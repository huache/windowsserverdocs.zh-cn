---
title: Active Directory 域服务的容量规划
description: AD DS 的容量规划过程中要考虑的因素的详细讨论。
ms.topic: article
ms.author: v-tea
author: teresa-motiv
ms.date: 7/3/2019
ms.openlocfilehash: 67b98988866fdc7a35e35ae353b7e37f3a13e083
ms.sourcegitcommit: 7cacfc38982c6006bee4eb756bcda353c4d3dd75
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 09/14/2020
ms.locfileid: "90077674"
---
# <a name="capacity-planning-for-active-directory-domain-services"></a>Active Directory 域服务的容量规划

本主题最初由 Ken Brumfield、Microsoft 的高级顶级现场工程师编写，并为 Active Directory 域服务 (AD DS) 提供容量规划建议。

## <a name="goals-of-capacity-planning"></a>容量规划目标

容量规划不同于对性能事件进行故障排除。 它们密切相关，但差别很大。 容量规划的目标是：

- 正确实现和操作环境
- 尽量缩短排查性能问题所花费的时间。

在容量规划中，在高峰期，组织的基准目标为40% 的处理器利用率，以满足客户端性能要求，并适应升级数据中心中的硬件所需的时间。 然而，若要获得异常性能事件的通知，可以在5分钟的时间间隔内将监视警报阈值设置为90%。

不同之处在于，在) 容量管理阈值不断超出 (一次性事件不是关注，增加容量 (也就是说，添加更多或更快的处理器) 会成为解决方案，或者跨多个服务器缩放服务将是一个解决方案。 性能警报阈值表示客户端体验当前正在受到处理，需要立即采取措施来解决问题。

类似的是，容量管理是指防止汽车意外 (防御性驱动，确保 brakes 正常工作，并如此) ，而性能故障排除是警察、防火部门和紧急医疗专员在发生事故后所做的事情。 这就是 "防御性驱动" Active Directory 样式。

过去几年来，扩展系统的容量规划指南发生了重大变化。 系统体系结构中的以下更改对设计和扩展服务的基本假设有挑战：

- 64位服务器平台
- 虚拟化
- 增加了功率消耗
- SSD 存储
- 云方案

此外，该方法从基于服务器的容量规划练习转移到基于服务的容量规划练习。 Active Directory 域服务 (AD DS) ，一种成熟的分布式服务，许多 Microsoft 和第三方产品用作后端，就成为了正确计划的最关键产品之一，以确保其他应用程序运行所需的容量。

### <a name="baseline-requirements-for-capacity-planning-guidance"></a>容量规划指南的基线要求

在本文中，需要满足以下基准要求：

- 读者已阅读并熟悉 [Windows Server 2012 R2 的性能优化指南](/previous-versions/dn529133(v=vs.85))。
- Windows Server 平台是基于 x64 的体系结构。 即使 Active Directory 环境安装在 Windows Server 2003 x86 上 (现在超出了支持生命周期的结束时间) ，并具有 (DIT) 的目录信息树，该树的大小低于 1.5 GB，并且可以轻松地保存在内存中，本文中的准则仍适用。
- 容量规划是一个连续的过程，应定期查看环境符合预期的情况。
- 硬件成本发生变化时，优化将在多个硬件生命周期内进行。 例如，内存成本更低，每个核心的成本降低，或不同存储选项的价格发生变化。
- 规划每天的高峰期。 建议在30分钟或小时间隔内查看此项。 任何更高的值可能会隐藏实际高峰，并且 "暂时性高峰" 可能会降低任何值。
- 规划企业硬件生命周期的增长。 这可能包括以分散的方式升级或添加硬件的策略，或每三到五年完全刷新一次。 每个都需要 "推测"，因为 Active Directory 上的负载会增长。 如果收集，历史数据将有助于进行此评估。
- 规划容错。 一旦评估 *n* 派生，就会规划包含 *n* &ndash; 1、 *n* &ndash; 2、 *n* &ndash; *x*的方案。
  - 根据组织的需要添加其他服务器，以确保丢失单个或多个服务器不会超过最大峰值容量估计。
  - 还应考虑发展计划和容错计划需要集成。 例如，如果需要一个 DC 来支持负载，但估计值为下一年的负载会加倍，并且需要两个 Dc 总计，则没有足够的容量来支持容错。 解决方案将从三个 Dc 开始。 如果预算紧张，还可能计划在3到6个月之后添加第三个 DC。

    > [!NOTE]
    > 添加 Active Directory 感知的应用程序可能会对 DC 负载产生明显影响，无论负载是来自应用程序服务器还是来自客户端。

### <a name="three-step-process-for-the-capacity-planning-cycle"></a>容量规划周期的三步过程

在容量规划中，首先确定需要的服务质量。 例如，核心数据中心支持较高级别的并发性，需要更一致的用户体验和使用应用程序，这需要更多地关注冗余并将系统和基础结构瓶颈降到最低。 与此相反，具有少量用户的附属位置不需要相同级别的并发或容错。 因此，在优化底层硬件和基础结构时，卫星办公室可能并不需要注意，这可能会导致成本节约。 此处提供的所有建议和指导均可获得最佳性能，可根据要求较低要求的方案选择性地放宽。

下一个问题是：虚拟化或物理？ 从容量规划角度来看，没有任何正确的答案;只使用一组不同的变量。 虚拟化方案分为以下两个选项之一：

- 每个主机一个来宾的 "直接映射" (其中虚拟化的存在仅用于从服务器抽象物理硬件) 
- "共享主机"

测试和生产方案表明可以将 "直接映射" 方案视为与物理主机相同的情况。 不过，"共享主机" 会引入更详细的一些注意事项。 "共享主机" 方案意味着 AD DS 也争用资源，在此过程中，还会产生处罚和优化注意事项。

考虑到这些注意事项，容量规划周期是一个迭代的三步过程：

1. 度量现有环境，确定系统瓶颈当前所在的位置，并获得规划所需容量的必要环境基础知识。
1. 根据步骤1中所述的条件确定所需的硬件。
1. 监视和验证实现的基础结构是否在规范内操作。 在此步骤中收集的某些数据成为了下一个容量计划周期的基线。

### <a name="applying-the-process"></a>正在应用进程

若要优化性能，请确保已正确选择这些主要组件，并将其调整为应用程序负载：

1. 内存
1. 网络
1. 存储
1. 处理器
1. Net Logon

由于几乎任何现代服务器类系统都将处理负载，因此，AD DS 的基本存储要求以及编写良好的客户端软件的常规行为会允许最多为10000到20000用户的环境在容量规划方面放弃大量投资。 也就是说，下表总结了如何评估现有环境以选择合适的硬件。 后续部分中将详细分析每个组件，以帮助 AD DS 管理员使用基线建议和特定于环境的主体评估其基础结构。

一般而言：

- 基于当前数据的任何大小调整都将仅对当前环境准确。
- 对于任何估计，都需要在硬件生命周期内增长。
- 确定今天是否太特大型并增长到更大的环境中，或者在生命周期中增加容量。
- 对于虚拟化，所有相同的容量规划主体和方法都适用，只需将虚拟化的开销添加到与域相关的内容。
- 容量规划（如试图预测的任何内容）不是精确的科学。 您不希望能用100% 的准确性来计算完美的。 此处的指导是 leanest 建议;增加容量以实现额外的安全性，并持续验证环境是否仍在目标上。

### <a name="data-collection-summary-tables"></a>数据收集摘要表

#### <a name="new-environment"></a>新的环境

| 组件 | 运往 |
|--|--|
| 存储/数据库大小 | 每个用户 40 KB 到 60 KB |
| RAM | 数据库大小<br />基本操作系统建议<br />第三方应用程序 |
| 网络 | 1 GB |
| CPU | 1000每个核心的并发用户 |

#### <a name="high-level-evaluation-criteria"></a>高级评估标准

| 组件 | 评估条件 | 规划注意事项 |
|--|--|--|
| 存储/数据库大小 | 在[存储限制](/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))中标题为 "激活由碎片整理释放的磁盘空间日志记录" 部分 |  |
| 存储/数据库性能 | <ul><li>"逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg disk sec/Read" "逻辑磁盘 (*\<NTDS Database Drive\>*) \avg Disk Sec/Write" "逻辑磁盘 (*\<NTDS Database Drive\>*) \avg Disk sec/Transfer"</li><li>"逻辑磁盘 (*\<NTDS Database Drive\>*) 读取数/秒，" "逻辑磁盘 (*\<NTDS Database Drive\>*) " 写入次数/秒 "" 逻辑磁盘 (*\<NTDS Database Drive\>*) 传输数/秒 "</li></ul> | <ul><li>存储有两个需要解决的问题<ul><li>可用空间，其大小为基于心轴和 SSD 的存储，这与大多数 AD 环境无关。</li> <li>输入/输出 (可用的 IO) 操作–在许多环境中，这通常是被忽略的。 但是，只需评估没有足够 RAM 来将整个 NTDS 数据库加载到内存中的环境，这一点非常重要。</li></ul><li>存储可能是一个复杂的主题，应涉及到适当大小的硬件供应商专业知识。 特别是对于更复杂的方案，例如 SAN、NAS 和 iSCSI 方案。 但是，一般情况下，每 Gb 存储的成本通常会直接反对派每个 IO 的成本：<ul><li>RAID 5 的每个千兆比 Raid 1 的成本更低，但 Raid 1 的每 IO 成本更低</li><li>基于纺锤的硬盘驱动器的每个千兆字节成本较低，但每个 IO 的 Ssd 成本更低</li></ul><li>重新启动计算机或 Active Directory 域服务服务后，可扩展存储引擎 (ESE) 缓存为空，并且在缓存得到预热时性能将是磁盘界限。</li><li>在大多数环境中，AD 是一种随机模式到磁盘的读取密集型 i/o，取消了缓存和读取优化策略的诸多好处。  此外，在内存中，AD 比大多数存储系统缓存具有更大的缓存方式。</li></ul> |
| RAM | <ul><li>数据库大小</li><li>基本操作系统建议</li><li>第三方应用程序</li></ul> | <ul><li>存储是计算机中速度最慢的组件。 RAM 中的内存越多，就越少需要再到磁盘。</li><li>确保分配了足够的 RAM 来存储操作系统、代理 (防病毒、备份、监视) 、NTDS 数据库和随着时间推移的增长。</li><li>对于最大程度地提高 RAM 数量的环境， (例如附属位置) 或不可行 (DIT 太大) ，请参考存储部分以确保存储大小正确。</li></ul> |
| 网络 | <ul><li>"网络接口 (\*) \ Received/sec"</li><li>" () 发送的网络接口数 \* /秒" | <ul><li>通常，从 DC 发送的流量远远超过发送到 DC 的流量。</li><li>交换以太网连接是全双工的，需要单独调整入站和出站网络流量。</li><li>合并 Dc 的数量将增加用于将响应发送回每个 DC 的客户端请求的带宽量，但对于整个站点而言，这将接近于线性。</li><li>如果删除卫星位置 Dc，请不要忘记将附属 DC 的带宽添加到中心 Dc，并使用它来评估有多少 WAN 流量。</li></ul> |
| CPU | <ul><li>"逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Read"</li><li>"进程 (lsass) \\ 处理器时间百分比"</li></ul> | <ul><li>消除存储作为瓶颈后，请解决所需的计算能力。</li><li>虽然不是完全线性的，但在特定范围内的所有服务器上使用的处理器内核数目 (如站点) 可用于确定支持客户端总负载所需的处理器数。 添加在范围内的所有系统中维护当前服务级别所需的最小值。</li><li>处理器速度的变化，包括与电源管理相关的更改、从当前环境派生的影响数值。 通常情况下，不可能精确评估从 2.5 GHz 处理器到 3 GHz 处理器的速度，从而减少了所需的 Cpu 数量。</li></ul> |
| NetLogon | <ul><li>"Netlogon (*) \Semaphore 获取" </li> <li> "netlogon (*) \semaphore 超时"</li><li>"Netlogon ( * ) \Average 信号灯 Hold Time"</li></ul> | <ul><li>Net Logon Secure 通道/MaxConcurrentAPI 仅影响具有 NTLM 身份验证和/或 PAC 验证的环境。 默认情况下，在 Windows Server 2008 之前的操作系统版本中启用 PAC 验证。 这是一种客户端设置，因此，在所有客户端系统上关闭此功能之前，Dc 将受到影响。</li><li>如果未正确调整大小，则具有大量交叉信任身份验证（包括林内信任）的环境具有更大的风险。</li><li>服务器合并将增加跨信任身份验证的并发性。</li><li>需要对电涌进行调整，例如群集故障转移，因为用户重新向新群集节点重新进行身份验证。</li><li>单个客户端系统 (例如群集) 可能需要进行优化。</li></ul> |

## <a name="planning"></a>规划

在很长一段时间内，社区建议的大小调整 AD DS 就是 "将其作为数据库大小的 RAM"。 在大多数情况下，建议的做法就是需要考虑的大多数环境。 但是，消耗 AD DS 的生态系统比 AD DS 环境本身更大，因为它在1999中进行了介绍。 尽管从 x86 体系结构增加计算能力和从 x86 体系结构到 x64 体系结构，但在物理硬件上运行 AD DS 的更大的一组客户之间不存在性能调整大小的微妙方面，但虚拟化的增长已将调整问题引入比以往更多的受众。

以下指南旨在确定和规划作为服务 Active Directory 的需求，而不考虑它是部署在物理、虚拟还是物理混合或纯粹的虚拟化方案中。 因此，我们会将评估细分为四个主要组件中的每个组件：存储、内存、网络和处理器。 简而言之，为了最大限度地提高 AD DS 性能，目标是尽可能接近处理器界限。

## <a name="ram"></a>RAM

简单地说，可以在 RAM 中缓存的空间越多，就越少需要再到磁盘。 为了最大限度地提高服务器的可伸缩性，RAM 的最小值应为当前数据库大小、总 SYSVOL 大小、操作系统建议数量和供应商建议 (防病毒、监视、备份等) 。 应该增加额外的容量以适应服务器生存期内的增长。 基于环境更改，这将基于数据库增长的估计值，以环保为依据。

对于最大程度地提高 RAM 数量的环境， (例如附属位置) 或不可行 (DIT 太大) ，请参考存储部分以确保正确设计存储。

在大小调整内存的一般上下文中出现的必然结果是页面文件的大小。 在与其他所有内存相关的上下文中，其目标是最大程度地减少到更慢的磁盘。 因此，问题应从 "如何调整页面文件大小？" "最大程度地减少分页所需的 RAM 量" 本部分的其余部分介绍了对后一问题的解答。 这就是将页面文件大小调整到一般操作系统建议领域的大部分讨论，并且需要为内存转储配置系统，这与 AD DS 性能无关。

### <a name="evaluating"></a>Evaluating

由于以下原因，域控制器 (DC) 需求的 RAM 量实际上是一种复杂的做法：

- 当尝试使用现有系统来衡量需要多少 RAM，而在内存压力条件下，由于人为 deflating 需要，可能会出现错误。
- 每个 DC 只需将 "有趣" 的客户端缓存到其客户端的主观事实。 这意味着，需要在只有 Exchange 服务器的站点中的 DC 上缓存的数据将与需要在仅对用户进行身份验证的 DC 上缓存的数据有很大的不同。
- 在环境发生变化时，针对每个 DC 计算 RAM 的劳动是不受制约的。
- 建议的条件将有助于做出明智的决策：
- 可在 RAM 中缓存的内存量越少，就越有必要再移到磁盘。
- 存储是指最慢的计算机组件。 对基于心轴和 SSD 存储媒体的数据的访问速度比在 RAM 中访问数据的1,000 倍慢。

因此，为了最大限度地提高服务器的可伸缩性，最小 RAM 为当前数据库大小、总 SYSVOL 大小、操作系统建议数量和供应商建议 (防病毒、监视、备份等) 。 添加额外数量以适应服务器生存期内的增长。 这将根据数据库增长的估计值，以环保为依据。 但是，对于包含一小部分最终用户的卫星位置，这些要求可能非常宽松，因为这些站点不需要缓存就可为大多数请求提供服务。

对于最大程度地提高 RAM 数量的环境， (例如附属位置) 或不可行 (DIT 太大) ，请参考存储部分以确保存储大小正确。

> [!NOTE]
> 调整内存大小时必然结果的大小是页面文件的大小。 由于目标是最大程度地减少进入速度变慢的磁盘，因此该问题来自 "应如何调整页面文件大小？" "最大程度地减少分页所需的 RAM 量" 本部分的其余部分介绍了对后一问题的解答。 这就是将页面文件大小调整到一般操作系统建议领域的大部分讨论，并且需要为内存转储配置系统，这与 AD DS 性能无关。

### <a name="virtualization-considerations-for-ram"></a>RAM 的虚拟化注意事项

避免主机上的内存过度提交。 优化 RAM 数量的根本目标是最大程度地减少在磁盘上花费的时间。 在虚拟化方案中，存在内存过度提交的概念，其中，将更多 RAM 分配给来宾，并在物理计算机上存在。 这本身也不是问题。 如果所有来宾主动使用的总内存超过主机上的 RAM 量，并且基础主机开始分页，则会出现问题。 如果域控制器转到 ntds.dit 以获取数据，或者域控制器转到页面文件以获取数据，或者主机正在进入磁盘以获取来宾认为在 RAM 中的数据，则性能将变为磁盘界限。

### <a name="calculation-summary-example"></a>计算摘要示例

| 组件 | 估计内存 (示例)  |
|--|--|
| Windows Server 2008) 的基本操作系统推荐 RAM ( | 2 GB |
| LSASS 内部任务 | 200 MB |
| 监视代理 | 100 MB |
| 防病毒 | 100 MB |
| 数据库 (全局编录)  | 8.5 GB |
| 要运行备份的缓冲，管理员无需影响即可登录 | 1 GB |
| 总计 | 12 GB |

**建议： 16 GB**

随着时间的推移，可以将更多数据添加到数据库中，而服务器可能会在生产中的3到5年内进行。 根据33% 的增长估计，16 GB 是要放入物理服务器的合理 RAM 量。 在虚拟机中，假设可以修改哪些设置，并且可以将 RAM 添加到 VM，则从 12 GB 开始，计划在将来进行监视和升级是合理的。

## <a name="network"></a>网络

### <a name="evaluating"></a>Evaluating

本部分的目的不是评估有关复制流量的需求，它侧重于遍历 WAN 的流量，并在 [Active Directory 复制流量](/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10))中完全涵盖，而不是评估所需的总带宽和网络容量，包括客户端查询、组策略应用程序等。 对于现有环境，可以使用性能计数器 "网络接口 (\*) \ Received/sec" 和 "Network interface (\*) \ Sent/sec" 来收集此情况。 用于15、30或60分钟的网络接口计数器的采样间隔。 对于良好的度量，通常情况下还会过于不稳定;任何更大的内容将在每日的每日平滑。

> [!NOTE]
> 通常，DC 上的大多数网络流量都是出站的，因为 DC 响应客户端查询。 这就是重点关注出站流量的原因，但建议同时评估入站流量的每个环境。 可以使用相同的方法来解决和查看入站网络流量需求。 有关详细信息，请参阅知识库文章 [929851： Tcp/ip 的默认动态端口范围在 Windows Vista 和 Windows Server 2008 中已更改](https://support.microsoft.com/kb/929851)。

### <a name="bandwidth-needs"></a>带宽需求

规划网络可伸缩性涉及两个不同的类别：流量和网络流量的 CPU 负载。 与本文中的某些其他主题相比，其中每个方案都是直前进的。

在评估必须支持多少流量时，有两种针对网络流量 AD DS 的容量规划的唯一类别。 第一种是在域控制器之间进行遍历并在引用 [Active Directory 复制流量](/previous-versions/windows/it-pro/windows-2000-server/bb742457(v=technet.10)) 中全面涵盖的复制流量，并且仍与 AD DS 的当前版本相关。 第二个是站点内客户端到服务器的通信。 规划的一种更简单的情况是，站点内流量主要接收来自客户端的小型请求，相对于发送回客户端的大量数据。 在站点中，100 MB 一般适用于每台服务器最多5000个用户。 对于超过5000用户的任何内容，建议使用 1 GB 网络适配器和接收方缩放 (RSS) 支持。 若要验证此方案，尤其是对于服务器合并方案，请查看 \* 站点中所有 dc 的网络接口 () \ Bytes/sec，将它们添加在一起，并将其除以域控制器的目标数目，以确保有足够的容量。 执行此操作的最简单方法是使用 Windows 可靠性和性能监视器中的 "堆积区域" 视图 (以前称为 Perfmon) ，确保所有计数器都按比例缩放。

请考虑以下示例 (也称为，一种真正复杂的方法是验证常规规则是否适用于特定环境) 。 进行了以下假设：

- 目标是尽可能减少服务器的占用量。 理想情况下，一台服务器将携带负载，并部署一个额外的服务器以实现冗余 (*N* + 1 方案) 。
- 在这种情况下，当前网络适配器仅支持 100 MB，在交换环境中。
  在 N 个方案中，最大目标网络带宽利用率为 60% (丢失 DC) 。
- 每台服务器都有大约10000的客户端连接到它。

从图表中的数据获取的知识 (网络接口 (\*) \ Sent/sec) ：

1. 营业日会在 7:00 PM 开始向下旋转5:30 和释放。
1. 最繁忙高峰期为 8:00 AM 到 8:15 AM，在最繁忙的 DC 上每秒发送的字节数大于25。
   > [!NOTE]
   > 所有性能数据均为历史记录。 因此，8:15 的高峰数据点表示从8:00 到8:15 的负载。
1. 4:00 AM 之前有波峰，在最繁忙的 DC 上每秒发送的字节数超过20个，这可能表示从不同的时区或后台基础结构活动（如备份）加载。 由于 8:00 AM 的高峰期超出了此活动，因此不相关。
1. 站点中有五台域控制器。
1. 最大负载约为每个 DC 5.5 MB/秒，这表示 100 MB 连接的44%。 使用此数据时，可以估计到 8:00 AM 到 8:15 AM 之间所需的总带宽为 28 MB/s。
   > [!NOTE]
   > 请小心，网络接口发送/接收计数器以字节为单位，网络带宽以位来度量。 100 MB &divide; 8 = 12.5 MB，1 GB &divide; 8 = 128 MB。

总结

1. 当前环境在60% 的目标利用率下满足 N + 1 级别的容错能力。 使一个系统脱机会将每台服务器的带宽从约 5.5 MB/秒转换 (44% ) 约 7 MB/秒 (56% ) 。
1. 根据前面所述的将合并到一台服务器的目标，这两者都超出了最大目标利用率，理论上可能使用了 100 MB 的连接。
1. 使用 1 GB 连接时，这将表示总容量的22%。
1. 在 " *N* + 1" 方案中的 "正常操作条件" 下，客户端负载会相对均匀地分布，每个服务器大约 14 MB/秒或总容量的11%。
1. 为了确保容量在 DC 不可用的情况下够用，每台服务器的正常操作目标约为30% 的网络利用率或每台服务器 38 MB/秒。 每台服务器的故障转移目标为60% 的网络利用率或 72 MB/秒。

简而言之，系统的最终部署必须有 1 GB 网络适配器，并连接到将支持所述负载的网络基础结构。 另外一条注意事项是，由于生成的网络流量量，网络通信的 CPU 负载可能会有重大影响，并限制 AD DS 的最大可伸缩性。 此过程可用于估计到 DC 的入站通信量。 但对于与入站流量相关的出站流量付诸实施，它是适用于大多数环境的学术练习。 确保 RSS 的硬件支持在每个服务器的用户数超过5000的环境中非常重要。 对于网络流量较高的方案，中断负载平衡可能会成为瓶颈。 处理器 (检测到这种情况， \*) \% 中断时间在 cpu 之间分布不均匀。 启用 RSS 的 Nic 可以减轻此限制并提高可伸缩性。

> [!NOTE]
> 类似的方法可用于估算合并数据中心或在附属位置停用域控制器时所需的额外容量。 只需收集发往客户端的出站和入站流量，就会成为 WAN 链接上现在会显示的流量。
>
> 在某些情况下，可能会遇到比预期更多的流量，因为流量较慢，如证书检查无法满足 WAN 上的高超时情况。 出于此原因，WAN 大小和利用率应为一个迭代的持续过程。

### <a name="virtualization-considerations-for-network-bandwidth"></a>网络带宽的虚拟化注意事项

为物理服务器提供建议非常简单：对于支持大于5000用户的服务器，为 1 GB。 在多个来宾开始共享基础虚拟交换机基础结构后，需要特别注意，以确保主机具有足够的网络带宽来支持系统上的所有来宾，并因此要求额外的严格。 这只是确保网络基础结构进入主机的扩展。 这是因为在网络流量通过虚拟交换机的主机上，网络是否包含作为虚拟机来宾运行的域控制器，或者是否直接连接到物理交换机。 虚拟交换机只是一个组件，上行需要支持传输的数据量。 因此，链接到交换机的物理主机物理网络适配器应该能够支持 DC 负载以及与连接到物理网络适配器的虚拟交换机共享的所有其他来宾。

### <a name="calculation-summary-example"></a>计算摘要示例

| 系统 | 最大带宽 |
|--|--|
| DC 1 | 6.5 MB/秒 |
| DC 2 | 6.25 MB/秒 |
| DC 3 | 6.25 MB/秒 |
| DC 4 | 5.75 MB/秒 |
| DC 5 | 4.75 MB/秒 |
| 总计 | 28.5 MB/秒 |

**建议： 72 mb/s** (28.5 mb/秒除以 40% ) 

|目标系统 (s) 计数|上述) 的总带宽 (|
|-|-|
|2|28.5 MB/秒|
|生成的正常行为|28.5 &divide; 2 = 14.25 MB/秒|

与往常一样，在这种情况下，客户端负载会增加，并且应尽可能最好地规划这一增长。 建议的计划数量为50% 的网络流量估计增长。

## <a name="storage"></a>存储

规划存储构成了两个组件：

- 容量或存储大小
- 性能

在规划容量上花费了大量的时间和文档，使性能经常被忽略。 使用最新的硬件成本，大多数环境都不够大，因为这两个环境都是需要考虑的问题，而建议 "将其放入尽可能多的 RAM 作为数据库大小" 通常会涉及到其他内容，尽管它可能会在更大的环境中多余。

### <a name="sizing"></a>调整大小

#### <a name="evaluating-for-storage"></a>评估存储

相比13年前，在引入 Active Directory 的情况下，一次最常见的驱动器大小为 4 GB，9 GB 驱动器，但对于除最大环境以外的所有环境，都不需要考虑 Active Directory 大小。 使用 180 GB 范围内的最小可用硬盘大小，整个操作系统、SYSVOL 和 ntds.dit 都可以轻松容纳在一个驱动器上。 因此，建议在此区域中弃用大量投资。

需要考虑的唯一建议是确保110% 的 ntds.dit 大小可用于启用碎片整理。 此外，应在硬件的整个生命周期内进行增长。

首先和最重要的注意事项是评估 ntds.dit 和 SYSVOL 的大小。 这些度量将导致固定磁盘和 RAM 分配的大小。 由于 (相对) 这些组件的成本较低，因此不需要严格和精确地进行数学计算。 有关如何评估现有环境和新环境的内容的内容，请参阅 [数据存储](/previous-versions/windows/it-pro/windows-2000-server/cc961771(v=technet.10)) 系列文章。 具体而言，请参阅以下文章：

- **对于现有环境 &ndash; **"[存储限制](/previous-versions/windows/it-pro/windows-2000-server/cc961769(v=technet.10))" 一文中的 "要激活由碎片整理释放的磁盘空间日志记录" 一节。
- **对于新环境 &ndash; **标题为 " [Active Directory 用户和组织单位的增长估计](/previous-versions/windows/it-pro/windows-2000-server/cc961779(v=technet.10))" 的文章。

  > [!NOTE]
  > 本文基于在 Windows 2000 中 Active Directory 发布时所做的数据大小估算。 使用反映环境中对象的实际大小的对象大小。

查看具有多个域的现有环境时，数据库大小可能有变化。 如果是这样，请使用最小的全局编录 (GC) 和非 GC 大小。

不同操作系统版本中的数据库大小可能有所不同。 运行早期操作系统的 Dc （如 Windows Server 2003）的数据库大小比运行更高版本操作系统（如 Windows Server 2008 R2）的 DC 小，尤其是在启用了 Active Directory 回收站或凭据漫游的功能时。

> [!NOTE]
>
>- 对于新的环境，请注意，Active Directory 用户和组织单位的增长估计中的估计值表明同一域中的100000用户 () 消耗约 450 MB 空间。 请注意，填充的属性可能会对总金额产生巨大影响。 第三方和 Microsoft 产品（包括 Microsoft Exchange Server 和 Lync）都将在多个对象上填充属性。 基于环境中产品组合的评估是首选的，但对数学和测试进行计算，并对所有不同的环境进行精确估计的操作并不是很值得花费的时间和精力。
>- 确保110% 的 ntds.dit 大小可用于启用脱机碎片整理，并计划在三到五年的硬件生命周期内增长。 考虑到存储空间的存储空间，在300% 的情况下估算存储空间，因为存储分配可安全地适应增长，并可能需要脱机碎片整理。

#### <a name="virtualization-considerations-for-storage"></a>存储的虚拟化注意事项

如果多个虚拟硬盘 (VHD) 文件分配到单个卷上，则应使用至少210% 的固定大小的磁盘 100 (% 的 DIT + 110% 可用空间) DIT 大小，以确保有足够的空间保留。

#### <a name="calculation-summary-example"></a>计算摘要示例

| 从评估阶段收集的数据 | 大小 |
|--|--|
| Ntds.dit 大小 | 35 GB |
| 允许脱机碎片整理的修饰符 | 2.1 GB |
| 需要的总存储空间 | 73.5 GB |

> [!NOTE]
> 需要进行此存储，以及 SYSVOL、操作系统、页面文件、临时文件、本地缓存数据 (（如安装程序文件) 和应用程序）所需的存储。

### <a name="storage-performance"></a>存储性能

#### <a name="evaluating-performance-of-storage"></a>评估存储的性能

作为任何计算机中最慢的组件，存储可能会对客户端体验产生最大的不利影响。 对于那些足以满足 RAM 大小建议并不可行的环境，忽视规划存储性能的后果可能会产生负面影响。  此外，复杂性和各种存储技术进一步提高了失败风险，因为在不同的物理磁盘上，"put 操作系统、日志和数据库" 的长期最佳实践的相关性在这种情况下很有用。  这是因为，时间较长的最佳做法是基于以下假设： "磁盘" 是专用心轴，这允许隔离 i/o。  实现此条件的这一假设不再与引入以下内容相关：

- 新存储类型和虚拟化和共享存储方案
- 存储区域网络上的共享心轴 (SAN) 
- SAN 或网络附加的存储上的 VHD 文件
- 固态驱动器
- 分层存储体系结构 (即 SSD 存储层缓存更大的基于主轴的存储) 

简而言之，无论底层存储体系结构和设计，所有存储性能工作的最终目标是确保每秒所需的输入/输出操作数量 (IOPS) 可用，并在可接受的时间范围内发生这些 IOPS。 本部分介绍如何评估底层存储的 AD DS 需求，以确保正确设计存储解决方案。  考虑到当今存储技术的可变性，最好与存储供应商合作，以确保有足够的 IOPS。  对于包含本地附加存储的方案，请参考附录 C，了解如何设计传统本地存储方案中的基础知识。  此主体通常适用于更复杂的存储层，还将帮助与支持后端存储解决方案的供应商对话。

- 由于提供了广泛的存储选项，因此建议将硬件支持团队或供应商的专业技能联系起来，以确保特定解决方案满足 AD DS 的需求。 以下数字是将向存储专家提供的信息。

对于数据库太大而无法保存到 RAM 中的环境，请使用性能计数器来确定需要支持多少 i/o：

- 逻辑磁盘 (\*) \Avg Disk sec/Read (例如，如果 ntds.dit 存储在 D：/驱动器，则完整路径将为逻辑磁盘 (D： ) \Avg Disk sec/Read) 
- 逻辑磁盘 (\*) \Avg Disk sec/Write
- 逻辑磁盘 (\*) \Avg Disk sec/Transfer
- 逻辑磁盘 (\*) 读取数/秒
- 逻辑磁盘 (\*) 写入数/秒
- 逻辑磁盘 (\*) 传输/秒

应在15/30/60 分钟间隔内对这些请求进行采样，以基准当前环境的需求。

#### <a name="evaluating-the-results"></a>评估结果

> [!NOTE]
> 重点是对数据库进行读取操作，因为这通常是最苛刻的组件，因此可以通过使用逻辑磁盘 (*\<NTDS Log\>*) \Avg Disk sec/Write 和逻辑磁盘 (*\<NTDS Log\>*) /写入/秒) 来将相同的逻辑应用到日志文件：
>
> - 逻辑磁盘 (*\<NTDS\>*) \Avg Disk sec/Read 指示当前存储是否已充分调整大小。  如果结果大致等于磁盘类型的磁盘访问时间，逻辑磁盘 (*\<NTDS\>*) 读取/秒是有效的度量值。  检查后端存储的制造商规范，但逻辑磁盘 (*\<NTDS\>*) \Avg Disk sec/Read 的良好范围大致为：
>   - 7200–9到12.5 毫秒 (ms) 
>   - 10000–6到10毫秒
>   - 15000–4到6毫秒
>   - SSD –1到3毫秒
>   - > [!NOTE]
>     > 存在的建议表明，存储性能在15ms 到20毫秒 (降级，具体取决于源) 。  上述值与其他指导的不同之处在于上述值是正常操作范围。  其他建议是故障排除指南，以确定客户端体验何时会大幅降低并明显下降。  参考附录 C 了解更深入的说明。
> - 逻辑磁盘 (*\<NTDS\>*) 读取数/秒是正在执行的 i/o 量。
>   - 如果在 *\<NTDS\>* 后端存储的最佳范围内逻辑磁盘 () \Avg Disk sec/Read 处于最佳范围内，则 *\<NTDS\>* 可以直接使用逻辑磁盘 () 读取/秒来调整存储大小。
>   - 如果逻辑磁盘 (*\<NTDS\>*) \Avg Disk sec/Read 不在后端存储的最佳范围内，则根据以下公式，需要额外的 i/o： (逻辑磁盘 (*\<NTDS\>*) \avg Disk Sec/Read) &divide; (物理媒体磁盘访问时间) &times; (逻辑磁盘 (*\<NTDS\>*) \avg Disk sec/Read) 

注意事项：

- 请注意，如果使用最理想的 RAM 量配置了服务器，则用于规划的这些值将不准确。  它们将会错误地出现在高端，仍可用作最糟糕的情况。
- 添加/优化 RAM 具体会降低读取 i/o (逻辑磁盘的数量， () 读取次数 *\<NTDS\>* /秒。 这意味着存储解决方案可能不一定比最初计算的更可靠。  遗憾的是，与此常规语句更具体的内容是依赖于客户端负载的环保，无法提供常规指导。  最佳选择是在优化 RAM 后调整存储大小。

#### <a name="virtualization-considerations-for-performance"></a>性能的虚拟化注意事项

与前面的所有虚拟化讨论类似，这里的关键是确保底层共享的基础结构能够支持 DC 负载以及使用底层共享媒体的所有路径和其他资源。 无论物理域控制器是在 SAN、NAS 还是 iSCSI 基础结构上共享相同的基础媒体（作为其他服务器或应用程序），无论是使用传递访问共享基础媒体的 SAN、NAS 还是 iSCSI 基础结构的来宾，还是如果来宾使用本地或 SAN 上共享媒体上的 VHD 文件，、NAS 或 iSCSI 基础结构。 规划练习的全部工作就是确保底层媒体可支持所有使用者的总负载。

此外，从来宾的角度来看，由于必须遍历其他代码路径，因此，必须通过主机访问任何存储，对性能产生影响。 不令人吃惊的是，存储性能测试表明虚拟化对主机系统的处理器利用率所主观的吞吐量产生了影响 (参阅附录 A： CPU 大小调整条件) ，这显然受来宾所要求的主机资源影响。 这有助于在虚拟化方案中与处理需求有关的虚拟化注意事项 (参阅 [处理) 的虚拟化注意事项](#virtualization-considerations-for-processing) 。

更复杂的是，有多种不同的存储选项可用，它们都有不同的性能影响。 作为从物理迁移到虚拟的一项安全估算，请使用1.10 的乘数调整 Hyper-v 上虚拟化来宾的不同存储选项，例如传递存储、SCSI 适配器或 IDE。 在不同存储方案间传输时需要进行的调整与存储是本地、SAN、NAS 还是 iSCSI 无关。

#### <a name="calculation-summary-example"></a>计算摘要示例

确定正常运行条件下正常运行的系统所需的 i/o 数量：

- 在 *\<NTDS Database Drive\>* 高峰期的15分钟内，逻辑磁盘 () 传输/秒
- 若要确定超过基础存储容量的存储所需的 i/o 数量，请执行以下操作：
  >*所需 IOPS* = (逻辑磁盘 (*\<NTDS Database Drive\>*) \avg Disk sec/Read &divide; *\<Target Avg Disk sec/Read\>*) &times; 逻辑磁盘 (*\<NTDS Database Drive\>*) 读取/秒

| 计数器 | “值” |
|--|--|
| 实际的逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Transfer | 02秒 (20 毫秒)  |
| 目标逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Transfer | 01秒 |
| 可用 IO 更改的乘数 | 0.02 &divide; 0.01 = 2 |

| 值名称 | 值 |
|--|--|
| 逻辑磁盘 (*\<NTDS Database Drive\>*) 传输/秒 | 400 |
| 可用 IO 更改的乘数 | 2 |
| 高峰期所需的总 IOPS | 800 |

若要确定缓存需要准备好的速率，请执行以下操作：

- 确定预热缓存的最长可接受时间。 它是从磁盘加载整个数据库所需的时间，或者是在 RAM 中无法加载整个数据库的情况下，这是最大程度地填满 RAM 的时间。
- 确定数据库的大小，不包括空格。  有关详细信息，请参阅 [评估存储](#evaluating-for-storage)。
- 将数据库大小除以 8 KB;这将是加载数据库所需的总 IOs。
- 将 Io 总数除以定义时间范围内的秒数。

请注意，计算结果（虽然精确）将不会精确，因为如果 ESE 未配置为具有固定的缓存大小，则会逐出以前加载的页面，并且默认情况下 AD DS 使用可变缓存大小。

| 要收集的数据点 | 值 |
|--|--|
| 最长可接受时间 | 10分钟 (600 秒)  |
| 数据库大小 | 2 GB |

| 计算步骤 | 公式 | 结果 |
|--|--|--|
| 计算页面中数据库的大小 |  (2 GB &times; 1024 &times; 1024) = *数据库大小（KB）* | 2097152 KB |
| 计算数据库中的页数 | 2097152 KB &divide; 8 kb = *页数* | 262144页 |
| 计算完全预热缓存所需的 IOPS | 262144页 &divide; 600 秒 = *需要的 IOPS* | 437 IOPS |

## <a name="processing"></a>正在处理

### <a name="evaluating-active-directory-processor-usage"></a>评估 Active Directory 处理器使用情况

对于大多数环境，按照 "规划" 一节中所述正确调整了存储、RAM 和网络，管理处理能力的量将是最值得注意的组件。 评估所需的 CPU 容量有两个难题：

- 无论环境中的应用程序在共享服务基础结构中的行为是否良好，都将在创建更有效的 Microsoft Active Directory 的应用程序或从下级 SAM 调用迁移到 LDAP 调用一文中的 "跟踪昂贵且低效的搜索" 一节中进行讨论。

  在较大的环境中，这一点很重要，这一点很重要，那就是编码不良的应用程序可以降低 CPU 负载的波动，从其他应用程序中 "盗取" 大量 CPU 时间、有人为的驱动器容量需求，以及针对 Dc 的非均匀分布负载。
- 由于 AD DS 是一种分布式环境，其中包含大量的潜在客户端，因此估算 "单一客户端" 的费用是环保的，因为使用模式以及利用 AD DS 的应用程序的类型或数量。 简而言之，与网络部分相似，对于广泛的适用性，从评估环境中所需的总容量角度来看，这是更好的做法。

对于现有环境，由于前面讨论过存储大小，因此假设存储现在已正确调整大小，因此与处理器负载相关的数据是有效的。 重申一遍，确保系统中的瓶颈不是存储的性能至关重要。 当瓶颈存在并且处理器正在等待时，一旦删除瓶颈，就会出现空闲状态。  随着处理器等待状态的定义，按定义增加 CPU 使用率，因为它不再需要等待数据。 因此，收集性能计数器 "逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Read" 和 "Process (lsass) \\ % Processor Time"。 \\如果 "逻辑磁盘 () \Avg Disk sec/Read" 超过10到 15 ms，则 "进程 (lsass) % Processor Time" 中的数据会导致人为低 *\<NTDS Database Drive\>* ，这是 Microsoft 支持用于排查与存储相关的性能问题的一般阈值。 与之前一样，建议采样间隔为15、30或60分钟。 对于良好的度量，通常情况下还会过于不稳定;任何更大的内容将在每日的每日平滑。

### <a name="introduction"></a>简介

为了规划域控制器的容量规划，处理能力需要最大的关注和理解。 当调整系统大小以确保最高性能时，始终有一个组件是瓶颈，而在适当大小的域控制器中，这将是处理器。

类似于跨站点查看环境要求的网络部分，必须为所需的计算能力执行相同的操作。 与 "网络" 部分不同（其中可用的网络技术在很大程度上超出正常需求），请注意调整 CPU 容量大小。  作为任何环境（甚至是中等大小）;超过一千个并发用户的任何内容都可以在 CPU 上投入大量负载。

不幸的是，由于使用 AD 的客户端应用程序的可变性非常大，因此，每个 CPU 的用户一般估计 woefully 不适用到所有环境。 具体而言，计算要求服从用户行为和应用程序配置文件。 因此，每个环境需要单独调整大小。

#### <a name="target-site-behavior-profile"></a>目标站点行为配置文件

如前所述，在规划整个站点的容量时，目标是将设计的目标设定为具有 *N* + 1 容量设计，这种情况下，在高峰期发生一次系统故障时，将允许在合理的质量级别继续服务。 这意味着，在 "*N*" 方案中，所有框中的负载都应小于 100% (比高峰期更好 ) ，小于80%。

此外，如果站点中的应用程序和客户端使用最佳做法来查找域控制器 (也就是说，使用 [DsGetDcName 函数](/windows/win32/api/dsgetdc/nf-dsgetdc-dsgetdcnamea)) ，客户端应相对均匀地分布，因为存在任意数量的因素。

在下一个示例中，进行了以下假设：

- 站点中的五个 Dc 都有四个 Cpu。
- 工作时间内的总目标 CPU 使用率在正常操作条件下为 40% ( "*n* + 1" ) ，60% 否则 ( "*n*" ) 。 在非工作时间内，目标 CPU 使用率为80%，因为备份软件和其他维护应该会消耗所有可用的资源。

![CPU 使用情况图表](media/capacity-planning-considerations-cpu-chart.png)

分析每个 Dc 的图表中的数据 `(Processor Information(_Total)\% Processor Utility)` ：

- 在大多数情况下，负载会相对均匀地分布，这就是客户端使用 DC 定位符并具有正确编写的搜索时应执行的操作。
- 5分钟峰值为10%，一些大小为20%。 通常，除非它们会导致超出容量计划目标，否则调查这些并不值得。
- 所有系统的高峰期介于大约 8:00 AM 到 9:15 AM 之间。 从大约 5:00 AM 到约 5:00 PM 的平滑过渡，这通常意味着业务周期。 在 5:00 PM 和 4:00 AM 之间，每个机箱上的 CPU 使用率的随机峰值会超出容量规划注意事项。

  > [!NOTE]
  > 在管理良好的系统上，可能会有一些峰值是正在运行的备份软件、完全系统防病毒扫描、硬件或软件清单、软件或修补程序部署等。 由于这些目标不在高峰用户业务周期之内，因此不会超出目标。

- 由于每个系统约40%，并且所有系统都具有相同数量的 Cpu，因此，如果发生故障或脱机，则剩余系统将以估计的53% 运行 (系统 D 的40% 负载均匀地剥离并添加到系统 A 和系统 C 的现有40% 负载) 。 由于许多原因，这种线性假设并不完全准确，但可为仪表提供足够的准确性。

  **备用方案–** 两个域控制器运行于40%：一个域控制器出现故障，剩余的 CPU 估计为80%。 这远远超出了上述容量计划的阈值，还开始严重限制以上负载配置文件中显示的10% 到20% 的头空间量，这意味着高峰会在 "*N*" 方案期间将 DC 增长到90% 到100%，并且确实降低了响应能力。

### <a name="calculating-cpu-demands"></a>计算 CPU 需求

"进程 \\ % Processor Time" 性能对象计数器汇总了应用程序的所有线程在 CPU 上所花费的总时间，并除以系统时间的总时间。 这样做的结果是多 CPU 系统上的多线程应用程序可能超过100% 的 CPU 时间，并将其解释为与 "处理器信息 \\ % Processor Utility" 的解释不同。 在实践中，可将 "进程 (lsass) \\ % Processor Time" 视为在支持进程需求所需的100% 的 cpu 计数。 值200% 表示需要2个 Cpu，每个 Cpu 均为100%，以支持完全 AD DS 负载。 尽管在100% 的容量下运行的 CPU 是从消耗 Cpu 和能耗和能耗方面获得的成本最高的成本，但对于附录 A 中所述的多种原因，在多线程系统上进行更好的响应会在系统未在100% 运行时出现。

为了适应客户端负载中的暂时性高峰，建议以40% 和60% 的系统容量的高峰期为目标。 使用上面的示例，这意味着在 3.33 (60% 目标) 和 5 (40% 目标) Cpu AD DS (的 lsass 进程需要) 的。 应根据基本操作系统和所需的其他代理 (（如防病毒、备份、监视等) ）中的要求增加额外容量。 尽管需要根据每个环境来评估代理的影响，但可以在一个 CPU 的5% 到10% 之间进行估计。 在当前的示例中，这会在高峰期内建议在 3.43 (60% 目标) 和 5.1 (40% 目标) Cpu 是必需的。

执行此操作的最简单方法是使用 Windows 可靠性和性能监视器中的 "堆积区域" 视图 (perfmon) ，确保所有计数器都按比例缩放。

假设：

- 目标是尽可能减少服务器的占用量。 理想情况下，一台服务器会附带负载，增加额外的服务器 (*N* + 1 方案) 。

![所有处理器上的 lsass 进程 (的处理器时间图表) ](media/capacity-planning-considerations-proc-time-chart.png)

从图表中的数据获取的知识 (处理 (lsass) \\ 处理器时间百分比) ：

- 营业日开始增加大约7:00，并在 5:00 PM 下降。
- 最繁忙高峰期为 9:30 AM 到 11:00 AM。
  > [!NOTE]
  > 所有性能数据均为历史记录。 9:15 的峰值数据点表示从9:00 到9:15 的负载。
- 7:00 AM 之前有个高峰，这可能表示从不同的时区或后台基础结构活动（例如备份）加载。 由于 9:30 AM 的高峰期超出了此活动，因此不相关。
- 站点中有三个域控制器。

最大负载时，lsass 消耗约485% 的 CPU，或在100% 运行 4.85 Cpu。 如前所述，这意味着站点需要大约 12.25 Cpu 用于 AD DS。 对于后台进程，请将上述建议5% 添加到10%，这意味着，立即替换服务器需要大约12.30 到12.35 个 Cpu，才能支持相同负载。 现在需要在中分解环境估计值的增长。

### <a name="when-to-tune-ldap-weights"></a>何时优化 LDAP 权重

在某些情况下，应考虑优化 [LdapSrvWeight](/previous-versions/windows/it-pro/windows-2000-server/cc957291(v=technet.10)) 。 在容量规划的上下文中，当应用程序或用户负载未均匀平衡，或者基础系统未按功能平衡时，会执行此操作。 超出容量规划的原因超出了本文的范围。

优化 LDAP 权重有两个常见原因：

- PDC 模拟器是一个示例，该示例影响用户或应用程序加载行为不是均匀分布的每个环境。 由于某些工具和操作面向 PDC 模拟器，如组策略管理工具、身份验证失败的第二次尝试、信任建立等，因此 PDC 仿真器上的 CPU 资源可能比站点中的其他地方更严格。
  - 仅当 CPU 使用率明显不同以便减少 PDC 模拟器上的负载并增加其他域控制器上的负载时，这种情况下才有用，因为这样可以更均匀地分布负载。
  - 在这种情况下，请在 PDC 模拟器的50和75之间设置 LDAPSrvWeight。
- 具有不同 Cpu 计数的服务器 (和速度在站点中) 。  例如，假设存在 2 8 核心服务器和 1 4 核服务器。  最后一个服务器有一半的其他两个服务器处理器。  这意味着，一种很好的分布式客户端负载会增加四核盒的平均 CPU 负载，使其大致为八核盒的两倍。
  - 例如，2 8 核心盒将在40% 运行，而四核盒将于80%。
  - 另外，请考虑在此方案中丢失 1 8 核心的影响，尤其是四核机箱现在会被重载这一事实。

#### <a name="example-1---pdc"></a>示例 1-PDC

| 系统 | 使用默认值 | 新 LdapSrvWeight | 估计的新利用率 |
|--|--|--|--|
| DC 1 (PDC 模拟器) | 53% | 57 | 40% |
| DC 2| 33% | 100 | 40% |
| DC 3| 33% | 100 | 40% |

此处的问题是，如果已传输或获取 PDC 仿真器角色（特别是站点中的另一个域控制器），新的 PDC 仿真器会大幅增加。

使用 [目标站点行为配置文件](#target-site-behavior-profile)部分中的示例，假设站点中的所有三个域控制器都具有四个 cpu。 如果其中一个域控制器具有八个 Cpu，则会出现什么情况？ 有两个域控制器处于40% 的利用率，1% 的利用率为20%。 虽然这并不是很糟糕，但有机会更好地平衡负载。 利用 LDAP 权重来实现此目的。  示例方案为：

#### <a name="example-2---differing-cpu-counts"></a>示例 2-不同 CPU 计数

| 系统 | 处理器信息 \\  % &nbsp; 处理器实用程序 (_Total) <br />使用默认值 | 新 LdapSrvWeight | 估计的新利用率 |
|--|--|--|--|
| 4-CPU DC 1 | 40 | 100 | 30% |
| 4-CPU DC 2 | 40 | 100 | 30% |
| 8-CPU DC 3 | 20 | 200 | 30% |

不过，请注意这些方案。 如上所述，数学看起来非常不错，却非常适合于纸张。 但在本文中，规划 "*N* + 1" 方案是一个极其重要的方案。 必须为每个方案计算一台 DC 处于脱机状态所造成的影响。 在这种情况下，负载分配甚至是这样一种情况：为了确保在 "*N*" 方案期间60% 的负载，在所有服务器之间平均平衡负载，因为比率保持一致。 查看 PDC 模拟器优化方案，在一般情况下，如果用户或应用程序的负载不均衡，这种效果会有所不同：

| 系统 | 优化利用率 | 新 LdapSrvWeight | 估计的新利用率 |
|--|--|--|--|
| DC 1 (PDC 模拟器)  | 40% | 85 | 47% |
| DC 2 | 40% | 100 | 53% |
| DC 3 | 40% | 100 | 53% |

### <a name="virtualization-considerations-for-processing"></a>处理的虚拟化注意事项

需要在虚拟化环境中完成两层容量规划。 在主机级别，类似于之前用于域控制器处理的业务周期的标识，需要确定高峰期内的阈值。 由于对于主机上的主机计算机计划来宾线程的基础主体是相同的，因此，我们建议在基础主机上 AD DS 获取 CPU 的相同目标（40% 到60%）。 在下一层 "来宾" 层上，由于线程计划的主体没有发生更改，来宾内的目标仍保留在40% 到60% 范围内。

在直接映射的方案中，每个主机有一个来宾，对此点所做的所有容量规划都需要添加到基础主机操作系统的要求 (RAM、磁盘、网络) 。 在共享主机方案中，测试表明对基础处理器的效率产生了10% 的影响。 这意味着，如果站点在40% 的目标位置需要10个 Cpu，则建议在所有 "*N*" 来宾分配的虚拟 cpu 量应为11。 在混合使用物理服务器和虚拟服务器的站点中，此修饰符仅适用于 Vm。 例如，如果站点具有 "*N* + 1" 方案，则一个具有10个 cpu 的物理或直接映射服务器将与主机上具有11个 cpu 的一个来宾等效，为域控制器保留11个 cpu。

在分析和计算支持 AD DS 负载所需的 CPU 数量时，映射到可在物理硬件方面购买的内容的 Cpu 数量不必完全映射。 虚拟化无需向上舍入。 虚拟化降低了将计算能力添加到站点所需的工作量，让你能够轻松地将 CPU 添加到 VM。 这并不意味着需要准确评估所需的计算能力，以便在需要将额外的 Cpu 添加到来宾时，基础硬件可用。  与往常一样，请记住计划和监视需求的增长。

### <a name="calculation-summary-example"></a>计算摘要示例

| 系统 | 高峰 CPU |
|--|--|--|
| DC 1 | 120% |
| DC 2 | 147% |
| Dc 3 | 218% |
| 使用的总 CPU | 485% |

| 目标系统 (s) 计数 | 上述) 的总带宽 ( |
|--|--|
| 40% 目标需要的 Cpu | 4.85 &divide; 。 4 = 12.25 |

由于这一点的重要性，请 *记住计划增长*，这是重复的。 在今后三年中，假设50% 的增长，则此环境将需要 18.375 Cpu (12.25 &times; 1.5) 三年。 备用计划将在第一年之后查看，并根据需要增加额外容量。

### <a name="cross-trust-client-authentication-load-for-ntlm"></a>NTLM 的交叉信任客户端身份验证负载

#### <a name="evaluating-cross-trust-client-authentication-load"></a>评估交叉信任客户端身份验证负载

许多环境可能具有一个或多个受信任连接的域。 对于另一个不使用 Kerberos 身份验证的域中的标识的身份验证请求，需要使用域控制器的安全通道向目标域中的另一个域控制器或指向目标域的路径中的下一个域遍历信任。 使用域控制器可以对受信任域中的域控制器执行的安全通道的并发调用数由称为 **MaxConcurrentAPI**的设置控制。 对于域控制器，确保安全通道能够处理负载量的方法是使用以下两种方法之一完成：优化 **MaxConcurrentAPI** ，或者在林中创建快捷方式信任。 若要测量单个信任间的流量，请参阅 [如何使用 MaxConcurrentApi 设置对 NTLM 身份验证执行性能优化](https://support.microsoft.com/kb/2688798)。

在数据收集过程中，这种情况与所有其他方案相同，必须在一天的高峰期内收集数据，以便数据有用。

> [!NOTE]
> 林内和林间方案可能导致身份验证遍历多个信任，每个阶段都需要进行优化。

#### <a name="planning"></a>规划

默认情况下，有许多使用 NTLM 身份验证的应用程序，或在特定的配置方案中使用它。 应用程序服务器的容量和服务增加了活动客户端的数量。 还有一种趋势，即客户端将会话保持打开状态的时间有限，而不定期重新连接 (如电子邮件拉取同步) 。 高 NTLM 负载的另一个常见示例是需要对 Internet 访问进行身份验证的 web 代理服务器。

这些应用程序可能会导致 NTLM 身份验证的负载大幅增加，这可能会对 Dc 施加重大压力，特别是当用户和资源在不同的域中时。

有多种方法可以管理交叉信任负载，实际上，这种方法是在中结合使用，而不是在独占或方案中使用。 可能的选项包括：

- 通过查找用户在用户所在的同一域中使用的服务，减少交叉信任客户端身份验证。
- 增加可用的安全通道数。 这与林内和跨林流量相关，称为快捷方式信任。
- 优化 **MaxConcurrentAPI**的默认设置。

对于现有服务器上的优化 **MaxConcurrentAPI** ，公式为：

> *New_MaxConcurrentApi_setting* &ge; (*semaphore_acquires*  +  *semaphore_time*) &times; *average_semaphore_hold_time* &divide; *time_collection_length* time_collection_length

有关详细信息，请参阅 [知识库文章2688798：如何使用 MaxConcurrentApi 设置对 NTLM 身份验证执行性能优化](https://support.microsoft.com/kb/2688798)。

## <a name="virtualization-considerations"></a>虚拟化注意事项

无，这是操作系统优化设置。

### <a name="calculation-summary-example"></a>计算摘要示例

| 数据类型 | 值 |
|--|--|
| 信号灯 (最小值获取)  | 6161 |
| 信号灯获取 (最大)  | 6762 |
| 信号量超时 | 0 |
| 平均信号灯持有时间 | 0.012 |
| 收集持续时间 (秒)  | 1:11 分钟 (71 秒)  |
| 从 KB 2688798) 的公式 ( |  ( # B1 6762 &ndash; 6161) + 0) &times; 0.012/ |
| **MaxConcurrentAPI**的最小值 |  ( # B1 6762 &ndash; 6161) + 0) &times; 0.012 &divide; 71 =。101 |

对于此时间段，默认值是可接受的。

## <a name="monitoring-for-compliance-with-capacity-planning-goals"></a>监视容量规划目标

在本文中，已讨论了规划和扩展对利用率目标的发展。 下面是建议阈值的摘要图表，必须进行监视以确保系统在足够的容量阈值内操作。 请记住，这些不是性能阈值，而是容量规划阈值。 在超过这些阈值的情况下，服务器将可以正常运行，但可以开始验证所有应用程序是否正常运行。 如果这种应用程序的行为良好，就可以开始评估硬件升级或其他配置更改。

| 类别 | 性能计数器 | 间隔/采样 | 目标 | 警告 |
|--|--|--|--|--|
| 处理器 | 处理器信息 (_Total) \\ 处理器实用程序 | 60 分钟 | 40% | 60% |
| RAM (Windows Server 2008 R2 或更早版本)  | Memory\Available MB | < 100 MB | 空值 | < 100 MB |
| RAM (Windows Server 2012)  | Memory\Long-Term 平均备用缓存生存期 (s)  | 30 分钟 | 必须测试 | 必须测试 |
| 网络 | 网络接口 (\*) 发送的 \/秒<p>网络接口 (\* 收到的) \/秒 | 30 分钟 | 40% | 60% |
| 存储 | 逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Read<p>逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Write | 60 分钟 | 10 毫秒 | 15 ms |
| AD 服务 | Netlogon (\*) \Average 信号灯时间 | 60 分钟 | 0 | 1 秒 |

## <a name="appendix-a-cpu-sizing-criteria"></a>附录 A： CPU 大小调整条件

### <a name="definitions"></a>定义

**处理器 (微处理器) –** 读取和执行程序指令的组件

**CPU –** 中央处理单元

**多核处理器–** 同一集成线路上的多个 cpu

**多 cpu –** 多个 cpu，而不是在同一集成线路上

**逻辑处理器-** 从操作系统的角度来看，一种逻辑计算引擎

这包括超线程、多核处理器上的一个核心或单核处理器。

由于今天的服务器系统具有多个处理器、多个多核处理器和超线程，因此此信息通用化以涵盖这两种方案。 因此，将使用术语 "逻辑处理器"，因为它表示可用计算引擎的操作系统和应用程序。

### <a name="thread-level-parallelism"></a>线程级并行度

每个线程都是独立的任务，因为每个线程都有自己的堆栈和说明。 由于 AD DS 是多线程的，因此可以通过使用 [Ntdsutil.exe在 Active Directory 中查看和设置 LDAP 策略 ](https://support.microsoft.com/kb/315071)来优化可用线程的数量，它可以在多个逻辑处理器之间很好地进行缩放。

### <a name="data-level-parallelism"></a>数据级别并行

这涉及到在一个进程中的多个线程之间共享数据， (在 AD DS 单独处理的情况下，) 在多个进程中的多个线程之间共享数据 (一般为) 。 考虑到过度简化事例，这意味着对数据所做的任何更改都会反映到所有不同级别的缓存中的所有正在运行的线程中， (L1，L2，L3) 跨所有运行所说线程的核心和更新共享内存。 在写入操作过程中，性能可能会降低，同时所有的内存位置都保持一致，然后才能继续进行指令处理。

### <a name="cpu-speed-vs-multiple-core-considerations"></a>CPU 速度与多核注意事项

一般的经验法则是，更快的逻辑处理器可减少处理一系列指令所用的持续时间，而更多逻辑处理器意味着可以同时运行更多任务。 这种情况下，这些经验法则会随着方案变得更加复杂，同时考虑到从共享内存中提取数据、等待数据级并行处理，以及管理多个线程的开销。 这也是多核系统中的可伸缩性不是线性的原因。

考虑以下这些注意事项中的类似于：考虑高速公路，每个螺纹为单个汽车，每个车道为一个核心，速度限制为时钟速度。

1. 如果高速公路上只有一个汽车，则有两个车道或12个车道并不重要。 这种汽车的速度只会尽可能快，因为速度限制会允许。
1. 假定线程需要的数据不会立即可用。 类比就是关闭路段。 如果高速公路上只有一个汽车，则在通道重新打开之前，速度限制是多少 (从内存) 中提取数据。
1. 随着汽车数量的增加，管理汽车数量的开销会增加。 当道路几乎为空时，请比较所需的体验和注意事项量 (例如，晚上晚上) 与流量繁忙 (如晚上，而不是) 高峰时间）。 另外，请考虑在两个通道高速公路上驱动时需要注意的事项，其中只有一个其他通道需要考虑驱动程序的作用，而不是六通道高速公路，其中一个需要考虑许多其他驱动程序的作用。
   > [!NOTE]
   > 下一节中扩展了有关高峰小时方案的比喻：响应时间/系统 Busyness 对性能的影响。

因此，有关更多或更快处理器的细节将对应用程序行为具有很大的主观度，在这种情况下，AD DS 是非常特定于环境的，甚至不同于从服务器到服务器内的服务器。 这就是本文前面提到的引用不太精确的原因，并且在计算中包括了安全性的利润。 做出预算驱动的购买决策时，建议先优化 40% (的处理器的使用情况，或者) 环境的所需数量，然后再考虑购买速度更快的处理器。 跨多个处理器的增加同步降低了线性行进中更多处理器的真正好处 (2 &times; 处理器数量提供的 &times; 可用额外计算能力) 2 个以上。

> [!NOTE]
> Amdahl 的法律和 Gustafson 的法律是这里的相关概念。

### <a name="response-timehow-the-system-busyness-impacts-performance"></a>响应时间/系统 busyness 对性能的影响

队列理论是 (队列) 的等待线的数学研究。 在队列理论中，利用率法由公式表示：

*U* k = *B* &divide; *T*

其中， *U* k 是利用率百分比， *B* 是繁忙的时间长度，而 *T* 是系统观察到的总时间。 转换为 Windows 上下文，这意味着处于运行状态的100纳秒 (ns) 间隔线程数除以在给定时间间隔内可用的 100-ns 间隔数。 这正是用于计算% Processor Utility (reference [Processor 对象](/previous-versions/ms804036(v=msdn.10)) 和 [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))) 的公式。

队列理论还提供公式： *n*  =  *U* k &divide; (1 &ndash; *U* k) 根据利用率 ( *n*为队列) 长度来估算等待项的数目。 按所有利用率间隔绘制的图表提供以下估计值：在给定的 CPU 负载中，处理器上的队列到达时间长短。

![队列长度](media/capacity-planning-considerations-queue-length.png)

观察到 50% CPU 负载后，每次队列中始终等待一个其他项目，并且在大约70% 的 CPU 使用率后，会显著增加速度。

返回到本部分前面使用的驱动类比：

- "下午中旬" 的繁忙时间将假设到40% 到70% 范围。 有足够的流量，以便选择任何通道的能力并不 majorly 限制，而另一台驱动程序的几率却很高，不需要在路上的其他汽车之间 "查找" 安全间隙。
- 其中一种方法会发现，由于流量接近高峰小时，路上系统会接近100% 的容量。 更改车道可能会变得非常富有挑战性，因为汽车非常接近，因此必须进行更多的小心。

这就是为什么在长周末) 后的第一天上午 (，在40% 的情况下，对在% 的 (情况下进行保守估计的平均容量估算允许使用头空间来应对负载) 不足的情况。

上面的语句提到% Processor Time 计算，与使用法相比，使用法是简化一般读者的简化。 对于数学上更为严格的：
- 转换 [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))
  - *B* = 100-ns 间隔 "空闲" 线程在逻辑处理器上花费的时间。 [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))计算的 "*X*" 变量中的更改
  - *T* = 在给定时间范围内的总 100-ns 时间间隔。 [PERF_100NSEC_TIMER_INV](/previous-versions/windows/embedded/ms901169(v=msdn.10))计算的 "*Y*" 变量中的更改。
  - *U* k = 逻辑处理器的利用率百分比，按 "空闲线程" 或% idle Time。
- 解决数学问题：
  - *U* k = 1 –处理器时间百分比
  - 处理器时间百分比 = 1 – *U* k
  - 处理器时间百分比 = 1 – *B*  /  *T*
  - % Processor Time = 1 – *X1* – *X0*  /  *Y1* – *Y0*

### <a name="applying-the-concepts-to-capacity-planning"></a>将概念应用于容量规划

前面的数学计算可能会对系统中所需的逻辑处理器数量充分复杂。 这就是为什么系统调整规模的方法侧重于基于当前负载确定最大目标利用率，并计算实现所需的逻辑处理器的数量。 此外，尽管逻辑处理器速度会对性能产生重大影响，但缓存效率、内存一致性要求、线程计划和同步以及生成均衡的客户端负载都将对性能有重大影响，这种情况会因服务器而异。 计算能力相对便宜，尝试分析并确定所需的 Cpu 数量就比提供业务价值要多得多。

40% 的要求并不难，而且这是一个合理的起点。 Active Directory 的各种使用者需要不同级别的响应能力。 在某些情况下，环境可以在80% 或90% 的利用率下运行，这是一种持续的平均值，因为对处理器的访问的等待时间将不会明显影响客户端性能。 重新迭代系统中的许多区域比系统中的逻辑处理器慢得多，这一点很重要，包括对 RAM 的访问、对磁盘的访问，以及通过网络传输响应。 所有这些项都需要进行调整。 示例：

- 将更多处理器添加到运行90% 的系统，该系统可能不会显著提高性能。 对系统进行更深入的分析可能会发现，有很多线程甚至无法在处理器上获得，因为它们正在等待 i/o 完成。
- 解决磁盘绑定问题可能意味着以前花费大量时间进入等待状态的线程将不再处于 i/o 的等待状态，因此，对于 CPU 时间，会有更多的竞赛，这意味着，上一示例中的90% 使用率将为 100% (，因为它不能更高) 。 这两个组件需要同时进行优化。
  > [!NOTE]
  > 处理器信息 ( * ) \\ % 处理器实用程序可能会超过100%，并且系统具有 "Turbo" 模式。  在这种情况下，CPU 会在短时间内超过额定的处理器速度。  参考 CPU 制造商文档和计数器说明了解更多见解。

讨论整个系统使用注意事项还会将会话域控制器作为虚拟化来宾。 [响应时间/系统 busyness 对性能的影响](#response-timehow-the-system-busyness-impacts-performance) ，适用于虚拟化方案中的主机和来宾。 这就是在只有一个来宾的主机中，域控制器 (，通常任何系统) 的性能接近于物理硬件上的性能。 向主机添加其他来宾会提高底层主机的利用率，从而增加等待时间以获取对处理器的访问权限（如前文所述）。 简而言之，需要在主机和来宾级别管理逻辑处理器利用率。

通过扩展以前的类似于，将高速公路作为物理硬件，会 analogized 来宾 VM，并 (一个快速总线，将直接转到附件需要) 的目标。 假设以下四种情况：

- 这是不是个小时，在几乎为空的总线上有一个附件，并且该总线的路上也几乎为空。 由于没有要与之通信的流量，因此附件的处理方式非常简单，并且与附件的处理速度一样快。 附件的旅行时间仍然受速度限制的限制。
- 它处于关闭状态，因此，总线几乎是空的，但路上的大多数车道已关闭，因此高速公路仍会拥塞。 附件在拥塞的路上会出现在近空的总线上。 虽然附件在总线上没有太多的竞赛，但在何处，总行程时间仍由外部流量决定。
- 这是高峰小时，因此高速公路和总线都堵塞。 此行程不仅需要更长时间，而且在总线上进入和关闭也是一种很大的麻烦，因为人们会从肩上获得肩，而高速公路并不那么好。 将更多的总线 (逻辑处理器添加到来宾) 并不意味着它们可以更轻松地放入路上，或将缩短行程。
- 但最后一种情况是，尽管它可能会将类比延伸到一小，但总线已满，但路上却没有堵塞。 尽管附件仍会在总线上出现问题，但在总线进入路上后，行程会提高效率。 这是将更多总线 (逻辑处理器添加到来宾) 将提高来宾性能的唯一方案。

从这个角度来看，在这种情况下，有很多方案需要在0% 利用率和100% 利用率的状态之间以及总线的0% 和100% 利用率状态之间产生不同程度的影响。

将高于 40% CPU 的主体应用为主机和来宾的合理目标是与上述相同的理由，这是队列的合理起点。

## <a name="appendix-b-considerations-regarding-different-processor-speeds-and-the-effect-of-processor-power-management-on-processor-speeds"></a>附录 B：有关不同处理器速度的注意事项，以及处理器电源管理对处理器速度的影响

在整个处理器选择部分中，假设处理器在每次收集数据时以100% 的速度运行，并且更换系统将具有相同的速度处理器。 尽管两个假设实际上都是 false （特别是在 Windows Server 2008 R2 和更高版本中，但在默认的电源计划 **平衡**的情况下），但该方法仍然是保守的方法。 尽管可能的错误率可能会增加，但它只会在处理器速度增加时增加安全性的利润。

- 例如，在需要 11.25 Cpu 的情况下，如果收集数据时处理器的运行速度非常快，则估计值可能为 5.125 &divide; 2。
- 不可能保证在给定的时间段内，时钟速度翻倍会加倍。 这是因为，处理器在 RAM 或其他系统组件上等待的时间量可能会保持不变。 最终效果是，更快的处理器可能会花费更长的空闲时间，同时等待获取数据。 同样，建议使用最低的常用分母，这是保守的，并避免尝试通过在处理器速度之间进行线性比较来计算可能的错误级别。

或者，如果更换硬件中的处理器速度低于当前硬件，则可以安全地增加按比例量计算所需的处理器的估计值。 例如，计算出需要10个处理器来维持站点中的负载，当前处理器运行在 3.3 Ghz，更换处理器将在 2.6 Ghz 运行，这是速度降低了21%。 在这种情况下，建议使用12个处理器。

也就是说，这种可变性不会更改容量管理处理器利用率目标。 由于处理器时钟速度将基于所要求的负载进行动态调整，因此，在较高的负载下运行系统将生成一个方案，其中 CPU 消耗更多的时间来提高时钟速度，最终目标是以40% 的速率在高峰速度处于 %100 的利用率。 如果任何内容小于 CPU 速度，则会在关闭高峰方案时降低性能。

> [!NOTE]
> 一个选项是在收集数据时，关闭处理器上的电源管理 (将电源计划设置为 **高性能**) 。 这样可以更准确地表示目标服务器上的 CPU 消耗。

若要调整不同处理器的估计值，使用此方法可以安全地处理，排除上面列出的其他系统瓶颈，以使处理器速度翻倍，这两倍于可执行的处理量。  目前，处理器的内部体系结构在处理器之间是不同的，这是一种更安全的方法来衡量使用不同于数据的处理器的效果，这是为了利用标准性能评估公司的 SPECint_rate2006 基准。

1. 查找正在使用的处理器的 SPECint_rate2006 分数以及要使用的计划。
    1. 在标准绩效评估公司的网站上，选择 " **结果**"，突出显示 " **CPU2006**"，然后选择 " **搜索所有 SPECint_rate2006 结果**"。
    1. 在 " **简单请求**" 下，输入目标处理器的搜索条件，例如 " **处理器匹配" e5-2630 (baselinetarget) ** 并且 " **处理器" 匹配 e5-2650 (基线) **。
    1. 查找要使用的服务器和处理器配置 (或关闭，如果不能找到完全匹配项) 并记下 " **结果** " 和 " **# 核心** " 列中的值。
1. 若要确定修饰符，请使用以下公式：
   > ( 第 B1 *个目标平台每核分数值*) &times; (*每核基线平台的每个内核数*) # A5 &divide; ( # B7 *基线每核分数值*) &times; (*每核目标平台*) # A11

    使用上面的示例：
   > (35.83 &times; 2000) &divide; (33.75 &times; 2300) = 0.92
1. 将处理器的估计数量乘以修饰符。  在上述示例中，若要从2650处理器到2630处理器，请将所需的计算所得的 11.25 Cpu &times; 0.92 = 10.35 处理器相乘。

## <a name="appendix-c-fundamentals-regarding-the-operating-system-interacting-with-storage"></a>附录 C：有关与存储交互的操作系统的基本知识

[响应时间/系统 busyness 对性能的影响](#response-timehow-the-system-busyness-impacts-performance)的队列理论概念也适用于存储。 熟悉操作系统如何处理 i/o 是应用这些概念所必需的。 在 Microsoft Windows 操作系统中，将为每个物理磁盘创建一个包含 i/o 请求的队列。 但是，需要对物理磁盘进行说明。 阵列控制器和 San 将主轴聚合作为单个物理磁盘提供给操作系统。 此外，阵列控制器和 San 可将多个磁盘聚合为一个阵列集，然后将此阵列集拆分为多个 "分区"，然后将该阵列集中作为多个物理磁盘提供给操作系统 (参考。图) 。

![块轴](media/capacity-planning-considerations-block-spindles.png)

在此图中，两个心轴会被镜像并分割为数据存储 (Data 1 和 Data 2) 的逻辑区域。 操作系统将这些逻辑区域视为单独的物理磁盘。

尽管这可能会造成很大的混乱，但在整个附录中使用以下术语来识别不同的实体：

- **磁盘轴–** 安装在服务器中的物理设备。
- **Array-** 由控制器聚合的心轴集合。
- **阵列分区–** 聚合数组的分区
- **LUN –** 在引用 san 时使用的阵列
- **磁盘–** 操作系统遵循的一个物理磁盘。
- **分区–** 操作系统视为物理磁盘的逻辑分区。

### <a name="operating-system-architecture-considerations"></a>操作系统体系结构注意事项

操作系统为观察到的每个磁盘创建第一个 In/First Out (FIFO) i/o 队列;此磁盘可能表示心轴、阵列或阵列分区。 从操作系统的角度来看，与处理 i/o 相比，活动队列越多越好。 对 FIFO 队列进行序列化时，必须按请求到达的顺序处理所有颁发给存储子系统的 i/o。 通过将操作系统观察到的每个磁盘与主轴/阵列关联，操作系统现在会为每个唯一的磁盘集维护一个 i/o 队列，从而消除跨磁盘的稀有 i/o 资源的争用，并将 i/o 需求隔离到单个磁盘。 作为一个例外，Windows Server 2008 引入了 i/o 优先顺序的概念，而设计为使用 "低" 优先级的应用程序将从该正常顺序中排除，并执行后退。 未专门对应用程序进行编码以利用 "低" 优先级默认为 "正常" 的应用程序。

### <a name="introducing-simple-storage-subsystems"></a>引入简单存储子系统

从一个简单的示例开始， (计算机内的单个硬盘驱动器) 将提供逐个组件分析。 将此分解为主要存储子系统组件，系统包含以下各项：

- **1 –** 10000 RPM 超高速度 scsi HD (超高速度 scsi 的传输速率为 20 MB/秒) 
- **1 –** SCSI 总线 (电缆) 
- **1 –** 超高速度 SCSI 适配器
- **1 –** 32 位 33 MHz PCI 总线

确定组件后，就可以计算出可以传输系统的数据量，或可以处理的 i/o 量。 请注意，可以传输系统的 i/o 数量和数据量是相关的，但不是相同的。 此相关取决于磁盘 i/o 是随机还是顺序，以及块大小。  (所有数据都作为块写入磁盘，但使用不同的块大小的不同应用程序。 ) 逐个组件：

- **硬盘–** 平均 10000 RPM 硬盘的 (毫秒为7毫秒，) 寻道时间和 3 ms 访问时间。 查找时间是读/写磁头移动到盘片上某个位置所需的平均时间量。 "访问时间" 是指将数据读取或写入到磁盘的平均时间，即头位于正确的位置。 因此，在 10000-RPM HD 中读取唯一数据块的平均时间是查找和访问，总共有大约10毫秒 (或每个数据块) .010 秒。

  当每个磁盘访问需要将头移动到磁盘上的新位置时，读/写行为称为 "随机"。 因此，当所有 i/o 均为随机 i/o 时，10000-RPM HD 每秒可处理大约100每秒的 i/o (IOPS)  (该公式为每秒1000毫秒除以每个 i/o 的10毫秒或 1000/10 = 100 IOPS) 。

  或者，当所有 i/o 都从 HD 上的相邻扇区发生时，这称为顺序 i/o。 顺序 i/o 没有查找时间，因为当第一次 i/o 完成时，读/写磁头位于数据的开始处，后者在硬盘上存储了下一个数据块。 因此，10000-RPM HD 能够处理每秒大约333个 i/o (每秒1000毫秒除以每个 i/o) 3 毫秒。

  > [!NOTE]
  > 此示例不反映磁盘缓存，其中一个圆柱的数据通常保留。 在这种情况下，第一个 i/o 需要10毫秒，磁盘读取整个柱面。 所有其他顺序 i/o 均从缓存中得到满足。 因此，磁盘中缓存可能会提高顺序 i/o 性能。

  到目前为止，硬盘驱动器的传输速率是不相关的。 无论硬盘驱动器为 20 MB/s 还是 Ultra3 160 MB/秒，10000-RPM HD 可处理的实际 IOPS 量是 ~ 100 随机或约300顺序 i/o。 由于块大小根据写入到驱动器的应用程序而变化，因此，每个 i/o 请求的数据量有所不同。 例如，如果块大小为 8 KB，则 100 i/o 操作将读取或写入硬盘，总计 800 KB。 但是，如果块大小为 32 KB，则 100 i/o 将读取/写入 3200 KB (3.2 MB) 驱动器。 只要 SCSI 传输速率超出传输的总数据量，就会获得 "更快" 传输速率驱动器。 请参阅下表以进行比较。

  | 说明 | 7200 RPM 9ms seek，4ms access | 10000 RPM 7ms seek，3ms access | 15000 RPM 4ms seek，2ms access |
  |--|--|--|--|
  | 随机 i/o | 80 | 100 | 150 |
  | 顺序 i/o | 250 | 300 | 500 |

  | 10000 RPM 驱动器 | 8 KB 块大小 (Active Directory Jet)  |
  |--|--|
  | 随机 i/o | 800 KB/秒 |
  | 顺序 i/o | 2400 KB/秒 |

- **SCSI 底板 (总线) –** 了解 "SCSI 底板 (总线) " 或在此方案中，如何影响存储子系统的吞吐量取决于块大小的知识。 实质上，问题是，如果 i/o 位于 8 KB 块中，总线句柄的 i/o 可以是多少？ 在此方案中，SCSI 总线为 20 MB/秒，即 20480 KB/秒。 20480 KB/s 除以 8 KB 块会产生最多 SCSI 总线支持的大约 2500 IOPS。

  > [!NOTE]
  > 下表中的数字表示一个示例。 大多数连接的存储设备当前使用 PCI Express，这提供了更高的吞吐量。

  | SCSI 总线每个块的大小受支持 | 2 KB 块大小 | 8 KB 块大小 (AD Jet)  (SQL Server 7.0/SQL Server 2000)  |
  |--|--|--|
  | 20 MB/秒 | 10,000 | 2,500 |
  | 40 MB/秒 | 20,000 | 5,000 |
  | 128 MB/秒 | 65,536 | 16,384 |
  | 320 MB/秒 | 160,000 | 40,000 |

  从此图表中可以确定，无论使用什么，总线都绝不会成为瓶颈，因为主轴最大值为 100 i/o，很好于以上任何阈值。

  > [!NOTE]
  > 这假设 SCSI 总线的效率为100%。

- **SCSI 适配器–** 为了确定此可以处理的 i/o 数量，需要检查制造商的规格。 将 i/o 请求定向到相应的设备需要处理某种排序，因此，可处理的 i/o 量取决于 SCSI 适配器 (或数组控制器) 处理器。

  在此示例中，将进行可处理 1000 i/o 的假设。

- **PCI 总线–** 这是一个经常被忽略的组件。 在此示例中，这并不是瓶颈;但随着系统的扩展，它可能成为瓶颈。 作为参考，在33Mhz 上运行的32位 PCI 总线在理论上传输 133 MB/秒的数据。 下面是公式：
  > 32位 &divide; 每字节 &times; 33 MHz = 133 MB/秒。

  请注意，是理论限制;事实上，实际上只达到了最大的50%，但在某些突发情况下，可以在短期内获得75% 的效率。

  66Mhz 64 位 PCI 总线可支持理论最大为每个字节 (64 位 &divide; 8 位 &times; （66 Mhz) = 528 MB/秒）。此外，任何其他 (设备（例如网络适配器、第二个 SCSI) 控制器等）都将减少带宽共享时可用的带宽，而设备会争用有限的资源。

分析此存储子系统的组件后，主轴是可请求的 i/o 数量中的限制因素，进而是可以传输系统的数据量。 具体来说，在 AD DS 方案中，这是每秒100个随机 i/o，以 8 KB 为增量为单位，每秒在访问 Jet 数据库时为每秒 800 KB。 或者，以独占方式分配给日志文件的心轴的最大吞吐量将受到以下限制：每秒300个顺序 i/o 以 8 KB 为增量，总计 2400 KB (每秒 2.4 MB) 。

现在，分析了简单配置，下表演示了在存储子系统中的组件发生更改或添加时瓶颈的发生位置。

| 备注 | 瓶颈分析 | 磁盘 | 总线 | 适配器 | PCI 总线 |
|--|--|--|--|--|--|
| 这是在添加第二个磁盘后的域控制器配置。 磁盘配置表示以 800 KB/s 为单位的瓶颈。 | 添加1个磁盘 (总计 = 2) <p>I/o 是随机的<p>4 KB 块大小<p>10000 RPM HD | 200 i/o 总数<br />总共 800 KB/秒。 |  |  |  |
| 添加7个磁盘后，磁盘配置仍表示瓶颈为 3200 KB/s。 | **添加7个磁盘 (总计 = 8) **  <p>I/o 是随机的<p>4 KB 块大小<p>10000 RPM HD | 800 i/o 总数。<br />总 3200 KB/秒 |  |  |  |
| 将 i/o 更改为顺序后，网络适配器成为瓶颈，因为它限制为 1000 IOPS。 | 添加7个磁盘 (总计 = 8) <p>**I/o 按顺序排列**<p>4 KB 块大小<p>10000 RPM HD |  |  | 2400可以读取/写入磁盘，控制器限制为 1000 IOPS |  |
| 将网络适配器替换为支持 10000 IOPS 的 SCSI 适配器后，瓶颈返回到磁盘配置。 | 添加7个磁盘 (总计 = 8) <p>I/o 是随机的<p>4 KB 块大小<p>10000 RPM HD<p>**升级 SCSI 适配器 (现在支持 10000 i/o) ** | 800 i/o 总数。<br />总 3200 KB/秒 |  |  |  |
| 将块大小增加到 32 KB 后，总线成为瓶颈，因为它仅支持 20 MB/s。 | 添加7个磁盘 (总计 = 8) <p>I/o 是随机的<p>**32 KB 块大小**<p>10000 RPM HD |  | 800 i/o 总数。 25600 KB/s (25 MB/s) 可以读写到磁盘。<p>总线仅支持 20 MB/秒 |  |  |
| 升级总线并添加更多磁盘后，磁盘仍会成为瓶颈。 | **添加13个磁盘 (总计 = 14) **<p>添加具有14个磁盘的第二个 SCSI 适配器<p>I/o 是随机的<p>4 KB 块大小<p>10000 RPM HD<p>**升级到 320 MB/s SCSI 总线** | 2800 i/o<p>11200 KB/s (10.9 MB/秒)  |  |  |  |
| 将 i/o 更改为连续后，磁盘仍处于瓶颈。 | 添加13个磁盘 (总计 = 14) <p>添加具有14个磁盘的第二个 SCSI 适配器<p>**I/o 按顺序排列**<p>4 KB 块大小<p>10000 RPM HD<p>升级到 320 MB/s SCSI 总线 | 8400 i/o<p>33600 KB\s<p> (32.8 MB\s)  |  |  |  |
| 添加更快的硬盘驱动器后，磁盘仍处于瓶颈。 | 添加13个磁盘 (总计 = 14) <p>添加具有14个磁盘的第二个 SCSI 适配器<p>I/o 按顺序排列<p>4 KB 块大小<p>**15000 RPM HD**<p>升级到 320 MB/s SCSI 总线 | 14000 i/o<p>56000 KB/秒<p> (54.7 MB/秒)  |  |  |  |
| 将块大小增加到 32 KB 后，PCI 总线会成为瓶颈。 | 添加13个磁盘 (总计 = 14) <p>添加具有14个磁盘的第二个 SCSI 适配器<p>I/o 按顺序排列<p>**32 KB 块大小**<p>15000 RPM HD<p>升级到 320 MB/s SCSI 总线 |  |  |  | 14000 i/o<p>448000 KB/秒<p> (437 MB/s) 是轴的读/写限制。<p>PCI 总线最大支持 133 MB/秒 (75%，最高) 。 |

### <a name="introducing-raid"></a>RAID 简介

引入阵列控制器时，存储子系统的性质不会显著变化;它仅替换计算中的 SCSI 适配器。 更改是在使用各种阵列级别 (如 RAID 0、RAID 1 或 RAID 5) 时，将数据读写到磁盘的代价。

在 RAID 0 中，数据跨 RAID 集中的所有磁盘进行条带化。 这意味着在执行读取或写入操作期间，会从每个磁盘中提取或推送数据的一部分，增加可在同一时间段内传输系统的数据量。 因此，在一秒钟内，每个心轴 (都假设 10000 RPM 驱动器) ，则可以执行100个 i/o 操作。 可支持的总 i/o 量为每个主轴每秒100个心轴，每秒生成 100 * N i/o)  (。

![逻辑 d：驱动器](media/capacity-planning-considerations-logical-d-drive.png)

在 RAID 1 中，对数据进行镜像 (跨一对轴上的重复) 以实现冗余。 因此，在执行读取 i/o 操作时，可以从该集中的两个轴上读取数据。 这样可以有效地在读取操作过程中提供两个磁盘的 i/o 容量。 要注意的是，在 RAID 1 中写入操作不会获得性能优势。 这是因为，为了实现冗余，需要将相同的数据写入两个驱动器。 尽管这两个轴在两个轴上同时发生，但它不需要更长的时间，因为这两个轴都是重复数据的，因此，中的写入 i/o 操作实际上会阻止发生两次读取操作。 因此，每次写入 i/o 都会产生两个读取 i/o 的成本。 可以根据该信息创建一个公式，以确定发生的 i/o 操作的总数：

> *读取 i/o* + 2 &times; *写入 i/o*  =  *总可用磁盘 i/o 总数*

如果读取数与写入数的比率已知，则可以从上述等式中派生出以下公式，以确定数组可支持的最大 i/o 数：

> *每个心轴* &times; 的最大 IOPS2个轴 &times; [ (*% 读取*  +  *% 写入%*) &divide; (*% 读取*+ 2 &times; *% 写入*) ] =*总 IOPS*

RAID 1 + 0 与 RAID 1 的行为完全相同，与读取和写入有关的费用相同。 但是，i/o 现在跨每个镜像集条带化。 如果

> *每个心轴* &times; 的最大 IOPS2个轴 &times; [ (*% 读取*  +  *% 写入%*) &divide; (*% 读取*+ 2 &times; *% 写入*) ] =*总 i/o*数

在 RAID 1 集中，当 RAID 1 集的多重性 (*N*) 为条带化时，可处理的总 i/o 将成为 &times; 每个 RAID 1 集的 N 个 i/o：

> *N* &times; {*每个主轴2轴的最大 IOPS* &times; &times; [ (*% 读取*  +  *% 写入%*) &divide; (*% 读取*+ 2 &times; *% 写入*) ]} =*总 IOPS*

在 RAID 5 中，有时称为 *n* + 1 RAID，数据跨 *n* 个轴条带化，奇偶校验信息写入 "+ 1" 主轴。 但是，与 RAID 1 或 1 + 0 相比，RAID 5 的开销要高得多。 每次向数组提交写入 i/o 时，RAID 5 都会执行以下操作：

1. 读取旧数据
1. 读取旧的奇偶校验
1. 写入新数据
1. 写入新的奇偶校验

由于操作系统提交给阵列控制器的每个写入 i/o 请求需要四个 i/o 操作才能完成，因此，提交的写入请求需要四次，长时间才能作为单个读 i/o 完成。 派生公式，将 i/o 请求从操作系统的角度转换为磁盘轴的经验：

> *读取 i/o* + 4 &times; *写入 i/o*  =  *总数*

与此类似，在 RAID 1 集中，当读取次数和数据轴的数量已知时，可以从上述等式派生，以确定数组可支持的最大 i/o 量 (请注意，主轴总数不包括奇偶校验) 丢失的 "驱动器"：

> *每纺锤* &times; 的 IOPS (*主轴*– 1) &times; [ (*% 读取*  +  *写入%*) &divide; (*% 读取*+ 4 &times; *% 写入*) ] =*总 IOPS*

### <a name="introducing-sans"></a>San 简介

扩展存储子系统的复杂性，将 SAN 引入到环境中时，所述的基本原则不会改变，但需要考虑所有连接到 SAN 的系统的 i/o 行为。 由于使用 SAN 的主要优势之一是，在内部或外部附加的存储上额外增加的冗余，容量规划现在需要考虑容错需求。 此外，还引入了需要评估的多个组件。 将 SAN 分解为组件部分：

- SCSI 或光纤通道硬盘
- 存储单元通道底板
- 存储单元
- 存储控制器模块
- SAN 交换机 (es) 
- HBA (s) 
- PCI 总线

在为冗余设计任何系统时，还包括了附加的组件以满足故障的可能性。 在容量规划中，从可用资源中排除冗余组件非常重要。 例如，如果 SAN 有两个控制器模块，则一个控制器模块的 i/o 容量全部都应该用于系统可用的总 i/o 吞吐量。 这是因为，如果一个控制器发生故障，则所有连接的系统所需的整个 i/o 负载都需要由剩余的控制器进行处理。 由于所有容量规划都是针对高峰使用时间进行的，因此不应将冗余组件分解为可用资源，计划的高峰利用率不应超过80% 的系统 (，以适应突发或异常系统行为) 。 同样，冗余 SAN 交换机、存储单元和心轴不应分解 i/o 计算。

分析 SCSI 或光纤通道硬盘驱动器的行为时，如上所述，分析行为的方法不会更改。 尽管每个协议都有一定的优点和缺点，但每个磁盘的限制因素是硬盘驱动器的机械限制。

分析存储单元上的通道与计算 SCSI 总线上可用的资源完全相同，或带宽 (如 20 MB/s) 除以块大小 (如 8 KB) 。 这与简单的上一个示例是在多个通道的聚合中。 例如，如果有6个通道，每个通道都支持 20 MB/秒的最大传输速率，则可用的 i/o 和数据传输总量为 100 MB/秒 (这是正确的，则不是 120 MB/秒) 。 同样，容错是此计算的主要玩家，在整个通道丢失的情况下，系统仅保留5个正常运行的通道。 因此，为了确保在发生故障时继续满足性能期望，所有存储通道的总吞吐量不应超过 100 MB/秒 (这种情况下，在所有通道) 之间平均分配负载和容错能力。 将此转换为 i/o 配置文件取决于应用程序的行为。 在 Active Directory Jet i/o 的情况下，这会与大约每秒12500个 i/o 相关，每秒 (100 MB/秒 &divide; 8) KB。

接下来，必须获得控制器模块的制造商规范，才能了解每个模块可以支持的吞吐量。 在此示例中，SAN 具有两个控制器模块，每个模块都支持 7500 i/o。 如果不需要冗余，系统的总吞吐量可能为 15000 IOPS。 在出现故障的情况下，在计算最大吞吐量时，限制为一个控制器的吞吐量或 7500 IOPS。 此阈值好于 12500 IOPS， (假设 4 KB 块大小) 最大值可受所有存储通道支持，因此当前是分析中的瓶颈。 仍出于规划目的，所需的最大 i/o 计划为10400。

当数据退出控制器模块时，它上流动的光纤通道连接以 1 GB/s (或每秒 1 GB) 进行评级。 若要将此与其他度量值相关联，1 GB/s 将变为 128 MB/s (1 GB/s &divide; 8 位/字节) 。 因为这超出了存储单元中所有通道的总带宽 (100 MB/秒) ，这不会给系统造成瓶颈。 此外，因为这只是两个通道中的一个 (额外的 1 GB/s 光纤通道连接为实现冗余) ，如果一个连接失败，剩余的连接仍有足够的容量来处理所需的所有数据传输。

如果路由到服务器，则数据最可能会传输 SAN 交换机。 由于 SAN 交换机必须处理传入的 i/o 请求并将其转发到相应的端口，因此交换机会对可处理的 i/o 数量有限制，但是，将需要制造商规范来确定限制。 例如，如果有两个交换机，每个交换机可以处理 10000 IOPS，则总吞吐量将是 20000 IOPS。 同样，如果一个交换机出现故障，系统的总吞吐量将为 10000 IOPS。 因为在正常操作中不需要超过80% 的使用率，所以使用不超过8000的 i/o 应为目标。

最后，安装在服务器中的 HBA 还会对它可以处理的 i/o 数量有限制。 通常，安装第二个 HBA 是为了实现冗余，但就像 SAN 交换机一样，在计算可处理的最大 i/o 时， *N* 1 个 hba 的总吞吐量是 &ndash; 系统最大的可伸缩性。

### <a name="caching-considerations"></a>缓存注意事项

缓存是在存储系统中的任何时间点可能会显著影响整体性能的组件之一。 有关缓存算法的详细分析超出了本文的讨论范围;然而，一些有关磁盘子系统上的缓存的基本语句值得一提：

- 缓存会改进持续顺序写入 i/o，因为它可以将多个较小的写入操作缓冲到更大的 i/o 块中，并在较少的块大小下转储到存储。 这将减少随机 i/o 总数和顺序 i/o 总数，从而为其他 i/o 提供更多的资源可用性。
- 缓存不会提高存储子系统的持续写入 i/o 吞吐量。 它仅允许对写入进行缓冲，直到可以使用主轴来提交数据。 当存储子系统中的主轴的所有可用 i/o 长时间饱和时，缓存最终将填满。 为了清空缓存，需要为在突发或额外的心轴之间留出足够的时间，以便提供足够的 i/o 以允许缓存刷新。

  较大的缓存仅允许缓冲更多数据。 这意味着可以容纳较长的饱和度。

  在通常的操作系统存储子系统中，操作系统会经历更高的写入性能，因为数据只需要写入缓存。 基础介质在 i/o 上饱和后，缓存将填满并写入性能将恢复到磁盘速度。

- 缓存读取 i/o 时，缓存最有利的情况是：数据按顺序存储在磁盘上，并且缓存可以预读 (这就假定下一扇区包含将在下一) 请求的数据。
- 当读取 i/o 是随机的时，在驱动器控制器上进行缓存不太可能对可以从磁盘读取的数据量提供任何增强。 如果操作系统或基于应用程序的缓存大小大于基于硬件的缓存大小，则不存在任何增强功能。

  在 Active Directory 的情况下，缓存仅受 RAM 量的限制。

### <a name="ssd-considerations"></a>SSD 注意事项

Ssd 与基于主轴的硬盘完全不同。 但仍有两个关键条件： "它可以处理多少 IOPS？" "这些 IOPS 的滞后时间是多少？" 与基于轴的硬盘相比，Ssd 可以处理更多的 i/o，并可能会降低延迟。 一般而言，在撰写本文时，虽然 Ssd 在成本上是按 Gb 的成本进行比较，但在存储性能方面，它们的成本非常低，因此非常经济。

注意事项：

- IOPS 和延迟对于制造商设计都非常主观，在某些情况下，在某些情况下，其性能比基于心轴的技术更差。 简而言之，按驱动器检查并验证制造商规范驱动器并不采用任何 generalities，这一点更重要。
- IOPS 类型可能有很大的数字，具体取决于它是读取还是写入。 一般情况下，AD DS 服务（通常是基于读取的）比其他应用程序方案更小。
- "写入耐用性" –这是 SSD 单元最终将磨损的概念。各种制造商应对这一挑战不同的 fashions。 最重要的是，对于数据库驱动器，主要的读取 i/o 配置文件允许轻描淡写此问题的重要性，因为数据不是非常稳定的。

### <a name="summary"></a>总结

考虑存储的一种方法是 picturing 的家庭管道。 想象一下，存储数据的介质的 IOPS 是家庭主排出。 如果这种情况堵塞 (例如，管道中的根) 或有限 (它已折叠或太小) ，则在使用的水太多时，家庭中的所有接收器 (的来宾数) 太多。 这完全类似于一个共享环境，在此环境中，一个或多个系统在具有相同基础媒体的 SAN/NAS/iSCSI 上利用共享存储。 可以采用不同的方法来解决不同的方案：

- 折叠或不太大的排水管需要完全缩放并修复。 这类似于在新硬件中添加新硬件，或在整个基础结构中使用共享存储重新分发系统。
- "堵塞" 管道通常表示识别一个或多个问题并删除这些问题。 在存储方案中，这可能是存储或系统级备份、跨所有服务器同步防病毒扫描以及高峰期运行的同步碎片整理软件。

在任何管道设计中，会将多个排出进到主排出。 如果任何内容会导致其中一个消耗点或一个交接点，则仅备份该交接点后面的内容。 在存储方案中，这可能是一种重载的交换机 (的 SAN/NAS/iSCSI 方案) 、驱动程序兼容性问题 (错误的驱动程序/HBA 固件/storport.sys 组合) 或备份/防病毒/碎片整理。 若要确定存储 "管道" 是否足够大，则需要测量 IOPS 和 i/o 大小。 每次将它们添加到一起，以确保有足够的 "管道直径"。

## <a name="appendix-d---discussion-on-storage-troubleshooting---environments-where-providing-at-least-as-much-ram-as-the-database-size-is-not-a-viable-option"></a>附录 D-讨论存储故障排除-在最大程度上提供与数据库大小相同的 RAM 的环境不是可行选项

了解为何存在这些建议，以便可以容纳存储技术中的更改时，这会很有帮助。 这些建议有两个原因。 第一种是隔离 IO，因此性能问题 (也就是说，操作系统主轴上的分页) 不影响数据库和 i/o 配置文件的性能。 第二个方面是 AD DS 的日志文件 (和大多数数据库) 在本质上是连续的，而与操作系统的更随机 i/o 模式相比，与操作系统的更随机 i/o 模式和几乎纯粹的随机 i/o 模式（如操作系统和 AD DS 数据库驱动器的几乎纯粹的 i/o 模式）一起使用时，基于磁盘轴的硬盘和缓存具有巨大的性能优势。 通过将顺序 i/o 隔离到单独的物理驱动器，可以增加吞吐量。 今天的存储选项带来的挑战是，这些建议背后的基本假设不再为 true。 在许多虚拟化存储方案中，如 iSCSI、SAN、NAS 和虚拟磁盘映像文件，基础存储媒体在多个主机之间共享，因此完全取消了 "IO 的隔离" 和 "顺序 i/o 优化" 方面的优势。 事实上，这些方案增加了另一层复杂性，因为访问共享媒体的其他主机可能会降低域控制器的响应能力。

在规划存储性能中，需要考虑三个类别：冷缓存状态、准备好缓存状态和备份/还原。 冷缓存状态发生在如下情况：例如，域控制器最初重新启动或重新启动 Active Directory 服务时，RAM 中没有 Active Directory 的数据。 温缓存状态是指域控制器处于稳定状态并且数据库已缓存。 需要特别注意的是，它们将驱动非常不同的性能配置文件，并且有足够的 RAM 来缓存整个数据库不会在缓存冷时帮助性能。 对于这两种方案，可以考虑使用以下类比进行性能设计：预热冷缓存是 "冲刺（sprint）"，运行具有预热缓存的服务器是 "marathon"。

对于冷缓存和热缓存方案，该问题会变得更快，存储可以将数据从磁盘移到内存。 预热缓存是一种方案，在此方案中，随着时间的推移，更多的查询重复使用数据、缓存命中率增加，以及需要使用磁盘减少的频率。 因此，磁盘降低对性能造成的影响。 在等待缓存热并增长到系统相关的最大允许大小时，性能下降只是暂时性的。 可以将会话简化为数据从磁盘中变得的速度，并简单度量可用于 Active Directory 的 IOPS，这对基础存储中提供的 IOPS 具有主观。 从规划的角度来看，因为预先计算缓存和备份/还原方案是一种例外情况，通常会在几小时内发生，但对于 DC 的负载而言，一般建议并不存在，但在不高峰时间计划这些活动的情况除外。

在大多数情况下，AD DS 主要是读取 IO，通常是90% 读/10% 写入的比率。 读 i/o 通常是用户体验和写入 IO 的瓶颈，导致写入性能下降。 由于 ntds.dit 的 i/o 主要是随机的，因此缓存倾向于最大程度地提高读取 IO 的优势，从而使正确配置读 i/o 配置文件的存储更重要。

对于正常的操作条件，存储规划目标是最大程度地减少从磁盘返回 AD DS 的请求的等待时间。 这实质上意味着未完成和挂起的 i/o 数小于或等于磁盘的路径数。 有多种方法可对此进行度量。 在性能监视方案中，常规建议是逻辑磁盘 (*\<NTDS Database Drive\>*) \Avg Disk sec/Read 小于 20 ms。 所需的操作阈值必须比存储的速度要低得多，但最好是尽可能接近存储速度，在2到6毫秒 ( .002 到 .006 的第二个) 范围，具体取决于存储类型。

示例：

![存储延迟图表](media/capacity-planning-considerations-storage-latency.png)

分析图表：

- **左侧绿色椭圆–** 滞后时间保持为10毫秒。 负载从 800 IOPS 增加到 2400 IOPS。 这是基础存储处理 i/o 请求的速度的绝对基底。 这取决于存储解决方案的具体内容。
- **右侧的 Burgundy 椭圆–** 从绿色圆圈的出口到数据收集结束的吞吐量仍保持不变，而延迟继续增加。 这表明，当请求卷超过基础存储的物理限制时，请求将在等待发送到存储子系统的队列中花费的时间越长。

应用此知识：

- 对**大型组成员资格的用户的影响–** 假设这需要读取磁盘中的 1 MB 数据、i/o 量和所需的时间，可按如下方式计算：
  - Active Directory 的数据库页的大小为 8 KB。
  - 至少需要从磁盘读取128页。
  - 假设未缓存任何内容，则在地板 (10 ms) 此操作将花费至少1.28 秒从磁盘加载数据，以便将数据返回到客户端。 在20毫秒后，存储上的吞吐量长时间极限，并且也是建议的最大值，将需要2.5 秒从磁盘获取数据，才能将数据返回给最终用户。
- **缓存将准备好的速率–** 假设客户端负载要使此存储示例的吞吐量最大化，则缓存将以每个 IO 的 2400 IOPS 为单位进行预热 &times; 。 或者大约每秒 20 MB/秒，每53秒将大约 1 GB 的数据库加载到 RAM 中。

> [!NOTE]
> 当组件主动读取或写入磁盘时，这种情况是正常的，例如当系统正在备份时，或在 AD DS 运行垃圾回收时。 应该提供更多的计算能力，以便适应这些定期事件。 目标是提供足够的吞吐量来满足这些方案，而不会影响正常函数。

可以看到，根据存储设计的情况，缓存可能会有多快。 这会使缓存的传入客户端请求数达到基础存储可以提供的比率。 在高峰期，将脚本运行到 "预先热" 的缓存，可为由真实的客户端请求所驱动的负载提供竞争。 这可能会对传递客户端所需的数据产生不利影响，因为在设计时，它将为稀有磁盘资源生成争用，因为尝试预热缓存会加载与 DC 联系的客户端不相关的数据。